{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auBwOusIB9lp"
      },
      "source": [
        "Previous Code: Drone Controller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCom1TgKCNpN"
      },
      "outputs": [],
      "source": [
        "group_number = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "5F_QYVQ3CNWH",
        "outputId": "fae4146c-78c1-4fda-e9c8-463d50df8e0a"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'cflib'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-1547373926.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcflib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrtp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcflib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrazyflie\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrazyflie\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcflib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrazyflie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cflib'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# This is an example from the Crazyflie Python API.\n",
        "# See https://github.com/bitcraze/crazyflie-lib-python/blob/master/examples/basiclogSync.py\n",
        "\n",
        "import logging\n",
        "import time\n",
        "\n",
        "import cflib.crtp\n",
        "from cflib.crazyflie import Crazyflie\n",
        "from cflib.crazyflie.log import LogConfig\n",
        "from cflib.crazyflie.syncCrazyflie import SyncCrazyflie\n",
        "from cflib.crazyflie.syncLogger import SyncLogger\n",
        "\n",
        "# Only output errors from the logging framework\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "\n",
        "# Initialize the low-level drivers (don't list the debug drivers)\n",
        "cflib.crtp.init_drivers(enable_debug_driver=False)\n",
        "# Scan for Crazyflies and use the first one found\n",
        "print('Scanning interfaces for Crazyflies...')\n",
        "available = cflib.crtp.scan_interfaces()\n",
        "print('Crazyflies found:')\n",
        "for i in available:\n",
        "    print(i[0])\n",
        "\n",
        "if len(available) == 0:\n",
        "    print('No Crazyflies found, cannot run example')\n",
        "else:\n",
        "    lg_stab = LogConfig(name='Stabilizer', period_in_ms=10)\n",
        "    lg_stab.add_variable('stabilizer.roll', 'float')\n",
        "    lg_stab.add_variable('stabilizer.pitch', 'float')\n",
        "    lg_stab.add_variable('stabilizer.yaw', 'float')\n",
        "\n",
        "    cf = Crazyflie(rw_cache='./cache')\n",
        "    with SyncCrazyflie(available[0][0], cf=cf) as scf:\n",
        "        with SyncLogger(scf, lg_stab) as logger:\n",
        "            endTime = time.time() + 10\n",
        "\n",
        "            for log_entry in logger:\n",
        "                timestamp = log_entry[0]\n",
        "                data = log_entry[1]\n",
        "                logconf_name = log_entry[2]\n",
        "\n",
        "                print('[%d][%s]: %s' % (timestamp, logconf_name, data))\n",
        "\n",
        "                if time.time() > endTime:\n",
        "                    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VAtJoPaBBzwI"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Code adapted from: https://github.com/bitcraze/crazyflie-lib-python/blob/master/examples/autonomousSequence.py\n",
        "\n",
        "import time\n",
        "# CrazyFlie imports:\n",
        "import cflib.crtp\n",
        "from cflib.crazyflie import Crazyflie\n",
        "from cflib.crazyflie.log import LogConfig\n",
        "from cflib.crazyflie.syncCrazyflie import SyncCrazyflie\n",
        "from cflib.crazyflie.syncLogger import SyncLogger\n",
        "\n",
        "## Some helper control functions:\n",
        "## -----------------------------------------------------------------------------------------\n",
        "\n",
        "# Determine initial position:\n",
        "def wait_for_position_estimator(scf):\n",
        "    print('Waiting for estimator to find position...')\n",
        "\n",
        "    log_config = LogConfig(name='Kalman Variance', period_in_ms=500)\n",
        "    log_config.add_variable('kalman.varPX', 'float')\n",
        "    log_config.add_variable('kalman.varPY', 'float')\n",
        "    log_config.add_variable('kalman.varPZ', 'float')\n",
        "\n",
        "    var_y_history = [1000] * 10\n",
        "    var_x_history = [1000] * 10\n",
        "    var_z_history = [1000] * 10\n",
        "\n",
        "    threshold = 0.001\n",
        "    with SyncLogger(scf, log_config) as logger:\n",
        "        for log_entry in logger:\n",
        "            data = log_entry[1]\n",
        "\n",
        "            var_x_history.append(data['kalman.varPX'])\n",
        "            var_x_history.pop(0)\n",
        "            var_y_history.append(data['kalman.varPY'])\n",
        "            var_y_history.pop(0)\n",
        "            var_z_history.append(data['kalman.varPZ'])\n",
        "            var_z_history.pop(0)\n",
        "\n",
        "            min_x = min(var_x_history)\n",
        "            max_x = max(var_x_history)\n",
        "            min_y = min(var_y_history)\n",
        "            max_y = max(var_y_history)\n",
        "            min_z = min(var_z_history)\n",
        "            max_z = max(var_z_history)\n",
        "\n",
        "            print(\"{} {} {}\".\n",
        "                format(max_x - min_x, max_y - min_y, max_z - min_z))\n",
        "\n",
        "            if (max_x - min_x) < threshold and (\n",
        "                    max_y - min_y) < threshold and (\n",
        "                    max_z - min_z) < threshold:\n",
        "                break\n",
        "\n",
        "# Initialize controller:\n",
        "def set_PID_controller(cf):\n",
        "    # Set the PID Controller:\n",
        "    print('Initializing PID Controller')\n",
        "    cf.param.set_value('stabilizer.controller', '1')\n",
        "    cf.param.set_value('kalman.resetEstimation', '1')\n",
        "    time.sleep(0.1)\n",
        "    cf.param.set_value('kalman.resetEstimation', '0')\n",
        "\n",
        "    wait_for_position_estimator(cf)\n",
        "    time.sleep(0.1)\n",
        "    return\n",
        "\n",
        "# Ascend and hover:\n",
        "def ascend_and_hover(cf):\n",
        "    # Ascend -- warmup drone:\n",
        "    for y in range(20):\n",
        "        cf.commander.send_hover_setpoint(0, 0, 0, y / 50)\n",
        "        time.sleep(0.1)\n",
        "    # Hover at 0.5 meters:\n",
        "    for _ in range(30):\n",
        "        #TODO: write the command to hover at .5 meters\n",
        "        cf.commander.send_hover_setpoint(0, 0, 0, 0.5)\n",
        "        time.sleep(0.1)\n",
        "    return\n",
        "\n",
        "# Follow the setpoint sequence trajectory:\n",
        "def run_sequence(scf, sequence, setpoint_delay):\n",
        "    cf = scf.cf\n",
        "    # TODO: write the for loop that loops over positions in the sequence\n",
        "    for position in sequence:\n",
        "        print(f'Setting position {(position[0], (position[1]))}')\n",
        "        # \"setpoint delay\" is a parameter to give the drone time to reach the set point\n",
        "        for i in range(setpoint_delay):\n",
        "            cf.commander.send_position_setpoint(position[0],\n",
        "                                                (position[1]),\n",
        "                                                0.5,\n",
        "                                                0.0)\n",
        "            time.sleep(0.1)\n",
        "\n",
        "# Hover, descend, and stop all motion:\n",
        "def hover_and_descend(cf):\n",
        "    # Hover at 0.5 meters:\n",
        "    for _ in range(30):\n",
        "        cf.commander.send_hover_setpoint(0, 0, 0, 0.5)\n",
        "        time.sleep(0.1)\n",
        "    # Descend:\n",
        "    for y in range(10):\n",
        "        cf.commander.send_hover_setpoint(0, 0, 0, (10 - y) / 25)\n",
        "        time.sleep(0.1)\n",
        "    # Stop all motion:\n",
        "    for i in range(10):\n",
        "        cf.commander.send_stop_setpoint()\n",
        "        time.sleep(0.1)\n",
        "    return\n",
        "\n",
        "def run_setpoint_trajectory(group_number, sequence):\n",
        "    # This is the main function to enable the drone to follow the trajectory.\n",
        "\n",
        "    # User inputs:\n",
        "    #\n",
        "    # - group_number: (int) the number corresponding to the drone radio settings.\n",
        "    #\n",
        "    # - sequence: a series of point locations (float) defined as a numpy array, with each row in the following format:\n",
        "    #     [x(meters), y(meters)]\n",
        "    #   Note: the input should be given in drone coordinates (where positive x is forward, and positive y is to the left).\n",
        "    # Example:\n",
        "    # sequence = [\n",
        "    #     [[ 0.          0.        ]\n",
        "    #      [0.18134891  0.08433607]]\n",
        "    #\n",
        "\n",
        "    # Outputs:\n",
        "    # None.\n",
        "\n",
        "    setpoint_delay = 3  # Number of 0.1s steps to spend at each setpoint\n",
        "\n",
        "    # Set the URI the Crazyflie will connect to\n",
        "    uri = f\"radio://0/{group_number}/2M\"\n",
        "\n",
        "    # Initialize Crazyflie radio drivers\n",
        "    cflib.crtp.init_drivers(enable_debug_driver=False)\n",
        "\n",
        "    # Connect to the drone and run the control loop\n",
        "    with SyncCrazyflie(uri, cf=Crazyflie(rw_cache='./cache')) as scf:\n",
        "        cf = scf.cf\n",
        "\n",
        "        # Initialize the PID controller and Kalman estimator\n",
        "        set_PID_controller(cf)\n",
        "\n",
        "        # TODO: Ascend to safe height before moving\n",
        "        ascend_and_hover(cf)\n",
        "\n",
        "        # TODO: Run the sequence of setpoints\n",
        "        run_sequence(scf, sequence, setpoint_delay)\n",
        "\n",
        "        # TODO: Descend and stop all motion\n",
        "        hover_and_descend(cf)\n",
        "\n",
        "    print(\"Done!\")\n",
        "    return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PThgAlHsCMA5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "class Drone_Controller:\n",
        "\n",
        "  def __init__(self, grid_width, rows, cols, group_number):\n",
        "      self.grid_width = grid_width\n",
        "      self.rows = rows\n",
        "      self.cols = cols\n",
        "      self.position = np.array([0,0])\n",
        "      self.group_number = group_number\n",
        "\n",
        "  def discretize_move(self, action, start_point, segments):\n",
        "\n",
        "    move_seq = np.zeros((segments+1,2))\n",
        "    cur = start_point\n",
        "    move_seq[0] = cur\n",
        "    increment = self.grid_width / segments\n",
        "\n",
        "    #TODO -- implement our discretizing strategy on a move!\n",
        "    # HINT: going \"up\" means positive in the x-value, \"left\" is positive in the y-value\n",
        "    #       Think about the Right hand rule!\n",
        "\n",
        "    if action == \"up\":\n",
        "      for i in range(1, segments + 1):\n",
        "        move_seq[i,0] = move_seq[i-1,0] + increment\n",
        "        move_seq[i,1] = move_seq[i-1,1]\n",
        "\n",
        "    elif action == \"down\":\n",
        "      for i in range(1, segments + 1):\n",
        "        move_seq[i,0] = move_seq[i-1,0] - increment\n",
        "        move_seq[i,1] = move_seq[i-1,1]\n",
        "\n",
        "    elif action == \"left\":\n",
        "      for i in range(1, segments + 1):\n",
        "        move_seq[i,0] = move_seq[i-1,0]\n",
        "        move_seq[i,1] = move_seq[i-1,1] + increment\n",
        "\n",
        "    elif action == \"right\":\n",
        "      for i in range(1, segments + 1):\n",
        "        move_seq[i,0] = move_seq[i-1,0]\n",
        "        move_seq[i,1] = move_seq[i-1,1] - increment\n",
        "    print(\"adding, \", move_seq)\n",
        "    return move_seq\n",
        "\n",
        "\n",
        "  def convert_traj_to_setpoint(self, policy, start, goal, segments, drone_init_pos):\n",
        "      \"\"\"\n",
        "      Converts a list of actions (up/down/left/right) into a sequence of setpoints.\n",
        "\n",
        "      Parameters:\n",
        "      - policy: policy recovered by QLearning\n",
        "      - start: grid position of start in the GridWorld, (x,y)\n",
        "      - goal: grid position of the end in the GridWorld, (x,y)\n",
        "      - segments: the amount by which we discretize each move\n",
        "      - drone_init_pos: the literal start point of the drone, usually (0,0)\n",
        "\n",
        "      \"\"\"\n",
        "      # TODO: make a trajectory and find a list of actions using trajectory.step()\n",
        "      trajectory = Trajectory(policy)\n",
        "      actions = trajectory.step(start, goal)\n",
        "\n",
        "      #initialize setpoints\n",
        "      setpoints = np.array(drone_init_pos).reshape((1,2))\n",
        "\n",
        "      # TODO: loop through the actions in our list, and calculate the next set of\n",
        "      #       setpoints. Add them to our setpoints using np.concatenate((array 1, array 2), axis = 0)\n",
        "      # HINT: we need to make sure we start each new setpoint from our last known position.\n",
        "      #       using the index [-1] gets us to the end of a list...\n",
        "      for a in actions:\n",
        "        print(\"calling disc move for action: \", a, \", from setpoint: \", setpoints[-1])\n",
        "        next_seg = self.discretize_move(a, setpoints[-1],segments)\n",
        "        setpoints = np.concatenate((setpoints, next_seg), axis = 0)\n",
        "\n",
        "      return setpoints\n",
        "\n",
        "  def convert_actions_to_setpoint(self, actions, segments, drone_init_pos):\n",
        "      setpoints = np.array(drone_init_pos).reshape((1,2))\n",
        "\n",
        "      # TODO: loop through the actions in our list, and calculate the next set of\n",
        "      #       setpoints. Add them to our setpoints using np.concatenate((array 1, array 2), axis = 0)\n",
        "      # HINT: we need to make sure we start each new setpoint from our last known position.\n",
        "      #       using the index [-1] gets us to the end of a list...\n",
        "\n",
        "      for a in actions:\n",
        "        print(\"calling disc move for action: \", a, \", from setpoint: \", setpoints[-1])\n",
        "        next_seg = self.discretize_move(a, setpoints[-1],segments)\n",
        "        setpoints = np.concatenate((setpoints, next_seg), axis = 0)\n",
        "\n",
        "      return setpoints\n",
        "\n",
        "\n",
        "  def execute_sequence(self, sequence):\n",
        "    run_setpoint_trajectory(self.group_number, sequence)\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUQwZcImCYx6"
      },
      "source": [
        "Previous Code for Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_26Rci1TChmW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6cc841b-64ea-4813-e317-9febd3714e96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting celluloid\n",
            "  Downloading celluloid-0.2.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from celluloid) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->celluloid) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->celluloid) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->celluloid) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->celluloid) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib->celluloid) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->celluloid) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->celluloid) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->celluloid) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->celluloid) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->celluloid) (1.17.0)\n",
            "Downloading celluloid-0.2.0-py3-none-any.whl (5.4 kB)\n",
            "Installing collected packages: celluloid\n",
            "Successfully installed celluloid-0.2.0\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "from typing import Tuple\n",
        "%pip install celluloid\n",
        "from celluloid import Camera # getting the camera\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from IPython.display import HTML\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.colors import TwoSlopeNorm, Normalize\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "### Visualization Tools ###\n",
        "\n",
        "### Visualization Tools ###\n",
        "\n",
        "class Move_anim:\n",
        "    \"\"\"\n",
        "    This class provides functionality for visualizing the motion of a trained\n",
        "    agent in real-time using matplotlib and a camera-like snapshot tool.\n",
        "\n",
        "    Args:\n",
        "        ax_obj (matplotlib axis): The axis to draw the animation on.\n",
        "        cam_obj (camera object): The camera used to capture frames (e.g., celluloid).\n",
        "        obs (list or np.ndarray): List of obstacle coordinates.\n",
        "        goal (list or np.ndarray): Goal coordinate.\n",
        "        bounds (list): Boundary coordinates.\n",
        "        grid_size (float): Size of each grid cell in the animation.\n",
        "        T (float): Time to animate a single move.\n",
        "        invert (bool): Whether to swap row/col in coordinate conversion.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ax_obj, cam_obj, obs, goal, bounds, grid_size=1, T=1, invert=False):\n",
        "        self.move_time = T\n",
        "        self.grid_size = grid_size\n",
        "        self.ax = ax_obj\n",
        "        self.camera = cam_obj\n",
        "        self.invert = invert\n",
        "        self._legend_drawn = False\n",
        "\n",
        "        # Ensure obs and goal are numpy arrays\n",
        "        self.obs = np.array(obs) if not isinstance(obs, np.ndarray) else obs\n",
        "        self.goal = np.array(goal) if not isinstance(goal, np.ndarray) else goal\n",
        "        self.bounds = bounds\n",
        "\n",
        "\n",
        "    def right(self, x, y):\n",
        "        \"Animate a rightward move\"\n",
        "        for i in range(int(self.move_time * 10)):\n",
        "            x += self.grid_size / (10 * self.move_time)\n",
        "            self.ax.scatter(x, y, c='black', edgecolors='white', s=120, marker='o')\n",
        "            self.show_obs()\n",
        "            self.camera.snap()\n",
        "        return x, y\n",
        "\n",
        "\n",
        "    def left(self, x, y):\n",
        "        \"Animate a leftward move\"\n",
        "        for i in range(int(self.move_time * 10)):\n",
        "            x -= self.grid_size / (10 * self.move_time)\n",
        "            self.ax.scatter(x, y, c='black', edgecolors='white', s=120, marker='o')\n",
        "            self.show_obs()\n",
        "            self.camera.snap()\n",
        "        return x, y\n",
        "\n",
        "\n",
        "    def up(self, x, y):\n",
        "        \"Animate an upward move\"\n",
        "        for i in range(int(self.move_time * 10)):\n",
        "            y -= self.grid_size / (10 * self.move_time)\n",
        "            self.ax.scatter(x, y, c='black', edgecolors='white', s=120, marker='o')\n",
        "            self.show_obs()\n",
        "            self.camera.snap()\n",
        "        return x, y\n",
        "\n",
        "\n",
        "    def down(self, x, y):\n",
        "        \"Animate a downward move\"\n",
        "        for i in range(int(self.move_time * 10)):\n",
        "            y += self.grid_size / (10 * self.move_time)\n",
        "            self.ax.scatter(x, y, c='black', edgecolors='white', s=120, marker='o')\n",
        "            self.show_obs()\n",
        "            self.camera.snap()\n",
        "        return x, y\n",
        "\n",
        "\n",
        "    def show_obs(self):\n",
        "        \"Show all static objects: obstacles, boundaries, and goal\"\n",
        "        def adapt_coords(pos):\n",
        "            row, col = pos\n",
        "            return (col + 0.5, row + 0.5) if self.invert else (row + 0.5, col + 0.5)\n",
        "\n",
        "        # Draw obstacles (red x)\n",
        "        for pos in self.obs:\n",
        "            x, y = adapt_coords(pos)\n",
        "            self.ax.scatter(x, y, c='red', marker='x', s=100)\n",
        "\n",
        "        # Draw boundaries (black x)\n",
        "        for pos in self.bounds:\n",
        "            x, y = adapt_coords(pos)\n",
        "            self.ax.scatter(x, y, c='black', marker='x', s=100)\n",
        "\n",
        "        # Draw goal (green star)\n",
        "        gx, gy = adapt_coords(self.goal)\n",
        "        self.ax.scatter(gx, gy, c='green', marker='*', s=200)\n",
        "\n",
        "        # Draw legend once\n",
        "        if not self._legend_drawn:\n",
        "            self.ax.scatter([], [], c='red', marker='x', s=100, label='Obstacle')\n",
        "            self.ax.scatter([], [], c='black', marker='x', s=100, label='Boundary')\n",
        "            self.ax.scatter([], [], c='green', marker='*', s=200, label='Goal')\n",
        "            self.ax.scatter([], [], c='black', edgecolors='white', s=120, marker='o', label='Agent')\n",
        "            self._legend_drawn = True\n",
        "\n",
        "    # Execute and animate a full trajectory from (cur_x, cur_y)\n",
        "    def execute_traj(self, traj, cur_x, cur_y):\n",
        "        # Optionally swap row and col based on display preference\n",
        "        if self.invert:\n",
        "            cur_x, cur_y = cur_y, cur_x\n",
        "\n",
        "        # Offset to center agent in the middle of a grid cell\n",
        "        x = cur_x + 0.5\n",
        "        y = cur_y + 0.5\n",
        "\n",
        "        # Move the agent step-by-step based on the trajectory list\n",
        "        for move in traj:\n",
        "            if move == \"right\":\n",
        "                x, y = self.right(x, y)\n",
        "            elif move == \"left\":\n",
        "                x, y = self.left(x, y)\n",
        "            elif move == \"up\":\n",
        "                x, y = self.up(x, y)\n",
        "            elif move == \"down\":\n",
        "                x, y = self.down(x, y)\n",
        "            else:\n",
        "                # No move (could handle invalid move here)\n",
        "                x, y = x, y\n",
        "\n",
        "        # Show legend after the full trajectory is played\n",
        "        self.ax.legend(loc='upper right', fontsize=10)\n",
        "\n",
        "\n",
        "\n",
        "### Method for visualizing q-table with directions corresponding to optimal actions for each state ###\n",
        "\n",
        "def draw_q_grid(q_table, env, scale=1.5, actions=[\"stay\", \"up\", \"down\", \"left\", \"right\"], focus_center=None, focus_size=3):\n",
        "    \"\"\"\n",
        "    Visualizes the Q-values of a grid-based environment using directional arrows and color-coded heatmaps.\n",
        "\n",
        "    Args:\n",
        "        q_table (dict): A dictionary with keys as (state, action) pairs and values as Q-values.\n",
        "        env (GridWorld): The grid environment (used for size and boundary info).\n",
        "        scale (float): Controls the visual scaling of the plot.\n",
        "        actions (list): List of action names corresponding to Q-values.\n",
        "        focus_center (tuple): Optional (x, y) center to zoom into a subsection of the grid.\n",
        "        focus_size (int): Size of the subsection to show if using focus_center.\n",
        "    \"\"\"\n",
        "\n",
        "    # === Determine region to visualize ===\n",
        "    if focus_center:\n",
        "        cx, cy = focus_center\n",
        "        half = focus_size // 2\n",
        "        row_range = range(max(0, cy - half), min(env.rows, cy + half + 1))\n",
        "        col_range = range(max(0, cx - half), min(env.cols, cx + half + 1))\n",
        "    else:\n",
        "        row_range = range(env.rows)\n",
        "        col_range = range(env.cols)\n",
        "\n",
        "    # Arrow offsets for directional visualization\n",
        "    dx, dy = 0.25, 0.25\n",
        "    arrow_dict = {\"U\": (0, -dy), \"D\": (0, dy), \"L\": (-dx, 0), \"R\": (dx, 0)}\n",
        "\n",
        "    # === Initialize plot ===\n",
        "    fig, ax = plt.subplots(figsize=(len(col_range) * scale, len(row_range) * scale))\n",
        "    ax.set_xlim(0, len(col_range))\n",
        "    ax.set_ylim(0, len(row_range))\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_xticks(np.arange(0, len(col_range)+1, 1))\n",
        "    ax.set_yticks(np.arange(0, len(row_range)+1, 1))\n",
        "    ax.invert_yaxis()\n",
        "    ax.grid(True)\n",
        "\n",
        "    # === Normalize Q-values for color mapping ===\n",
        "    all_q_vals = [v for (_, v) in q_table.items()]\n",
        "    min_q = min(all_q_vals) if all_q_vals else -1\n",
        "    max_q = max(all_q_vals) if all_q_vals else 1\n",
        "    if min_q < 0 and max_q > 0:\n",
        "        norm = TwoSlopeNorm(vmin=min_q, vcenter=0, vmax=max_q)\n",
        "    else:\n",
        "        norm = Normalize(vmin=min_q, vmax=max_q)\n",
        "    cmap = plt.cm.bwr  # Blue-White-Red colormap\n",
        "\n",
        "    # === Iterate over grid cells ===\n",
        "    for i, row in enumerate(row_range):\n",
        "        for j, col in enumerate(col_range):\n",
        "            state = (row, col)\n",
        "\n",
        "            # Get Q-values for each action at this state\n",
        "            q_stay  = q_table.get((state, \"stay\"), 0)\n",
        "            q_up    = q_table.get((state, \"up\"), 0)\n",
        "            q_down  = q_table.get((state, \"down\"), 0)\n",
        "            q_left  = q_table.get((state, \"left\"), 0)\n",
        "            q_right = q_table.get((state, \"right\"), 0)\n",
        "\n",
        "            # Cell center coordinates\n",
        "            x, y = j, i\n",
        "            cx, cy = x + 0.5, y + 0.5\n",
        "\n",
        "            # Identify boundaries\n",
        "            px, py = state\n",
        "            boundary = px == 0 or px == env.rows - 1 or py == 0 or py == env.cols - 1\n",
        "\n",
        "            # === Drawing helper functions ===\n",
        "            def draw_triangle(points, q_val):\n",
        "                color = 'black' if boundary else cmap(norm(q_val))\n",
        "                triangle = patches.Polygon(points, closed=True, facecolor=color, edgecolor='black', alpha=0.85)\n",
        "                ax.add_patch(triangle)\n",
        "\n",
        "            def draw_square(origin):\n",
        "                square = patches.Rectangle(origin, width=1, height=1, facecolor='black', edgecolor='black', alpha=1.0)\n",
        "                ax.add_patch(square)\n",
        "\n",
        "            def draw_stay_circle(center, q_val, radius=0.1):\n",
        "                color = 'black' if (0 in state) or (env.cols-1 in state) else cmap(norm(q_val))\n",
        "                circle = patches.Circle(center, radius, facecolor=color, edgecolor='black', alpha=0.9)\n",
        "                ax.add_patch(circle)\n",
        "\n",
        "            # === Render cell ===\n",
        "            if boundary:\n",
        "                draw_square((x, y))\n",
        "            else:\n",
        "                # Draw directional triangles for each action\n",
        "                draw_triangle([(x, y), (x+1, y), (cx, cy)], q_up)\n",
        "                draw_triangle([(x, y+1), (x+1, y+1), (cx, cy)], q_down)\n",
        "                draw_triangle([(x, y), (x, y+1), (cx, cy)], q_left)\n",
        "                draw_triangle([(x+1, y), (x+1, y+1), (cx, cy)], q_right)\n",
        "\n",
        "                # Draw circle for 'stay' action\n",
        "                draw_stay_circle((cx, cy), q_stay)\n",
        "\n",
        "                # Determine best action and annotate with direction\n",
        "                q_vals = [q_stay, q_up, q_down, q_left, q_right]\n",
        "                best_action = actions[np.argmax(q_vals)]\n",
        "                direction = best_action[0].upper()\n",
        "                ax.text(cx, cy, direction, ha='center', va='center', fontsize=10, color='black')\n",
        "\n",
        "                # Optional: draw directional arrow\n",
        "                if direction in arrow_dict:\n",
        "                    dx, dy = arrow_dict[direction]\n",
        "                    ax.arrow(cx, cy, dx, dy, width=0.006)\n",
        "\n",
        "    # === Add colorbar ===\n",
        "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "    sm.set_array([])  # Required for colorbar to work\n",
        "    cbar = plt.colorbar(sm, ax=ax)\n",
        "    cbar.set_label(\"Q-Value\")\n",
        "\n",
        "    # Final formatting\n",
        "    plt.title(\"Q-Value Heatmap\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9z08n06CipE"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "class Trajectory:\n",
        "  \"\"\"\n",
        "  This class provides functionality for extracting or rolling\n",
        "  out optimal trajectory from learned policy.\n",
        "\n",
        "  Args:\n",
        "      policy (dict): A dictionary with keys as state tuples and action strings as values.\n",
        "      action_step (dict): A dictionary with keys as action strings and values as step tuples\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,policy):\n",
        "    self.policy = policy\n",
        "    self.action_step = {\n",
        "        \"up\":(-1,0),\n",
        "        \"down\":(1,0),\n",
        "        \"right\":(0,1),\n",
        "        \"left\":(0,-1),\n",
        "        \"stay\":(0,0)\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "  def step(self, start_pos, goal_pos):\n",
        "    \"Recieves starting postion and goal position returns optimal action sequence\"\n",
        "    position = start_pos\n",
        "    actions = []\n",
        "    while not (position == goal):\n",
        "      action = self.policy.get(position)\n",
        "      actions.append(action)\n",
        "      print(action)\n",
        "      step = self.action_step.get(action)\n",
        "      position = tuple(map(sum,zip(position, step)))\n",
        "    return actions\n",
        "\n",
        "#Setup MDP problem\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### === Agent === ###\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    Grid-based agent that can move in 4 directions or stay in place.\n",
        "    \"\"\"\n",
        "    def __init__(self, action_space=[\"stay\", \"up\", \"down\", \"left\", \"right\"]):\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def reset(self, start_pos):\n",
        "        \"\"\"Reset agent to a starting position.\"\"\"\n",
        "        self.position = start_pos\n",
        "\n",
        "    def move(self, action, env: GridWorld):\n",
        "        \"\"\"\n",
        "        Move the agent based on the chosen action, considering environment boundaries.\n",
        "        \"\"\"\n",
        "        row, col = self.position\n",
        "        if action == \"up\" and row > 0:\n",
        "            row -= 1\n",
        "        elif action == \"down\" and row < env.rows - 1:\n",
        "            row += 1\n",
        "        elif action == \"left\" and col > 0:\n",
        "            col -= 1\n",
        "        elif action == \"right\" and col < env.cols - 1:\n",
        "            col += 1\n",
        "        # \"stay\" does nothing\n",
        "        self.position = (row, col)\n",
        "        return self.position\n",
        "\n",
        "### === Q-Learning Algorithm === ###\n",
        "class QLearning:\n",
        "    \"\"\"\n",
        "    Tabular Q-learning agent for discrete state-action environments.\n",
        "    \"\"\"\n",
        "    def __init__(self, env: GridWorld, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.alpha = alpha                  # Learning rate\n",
        "        self.gamma = gamma                  # Discount factor\n",
        "        self.epsilon = epsilon              # Exploration rate\n",
        "        self.q_table = {}                   # Stores Q-values: (state, action) -> float\n",
        "        self.actions = [\"stay\", \"up\", \"down\", \"left\", \"right\"]\n",
        "        self.env = env\n",
        "\n",
        "    def get_q(self, state, action):\n",
        "        \"\"\"Return Q-value for a given state-action pair; default to 0.0.\"\"\"\n",
        "        return self.q_table.get((state, action), 0.0)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"\n",
        "        Choose an action using epsilon-greedy strategy.\n",
        "        With probability epsilon, explore; otherwise exploit.\n",
        "        \"\"\"\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(self.actions)\n",
        "        q_vals = [self.get_q(state, a) for a in self.actions]\n",
        "        return self.actions[np.argmax(q_vals)]\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"\n",
        "        Perform the Q-learning update:\n",
        "        Q(s, a) ← Q(s, a) + α [r + γ max_a' Q(s', a') − Q(s, a)]\n",
        "        \"\"\"\n",
        "\n",
        "        max_q_next = max([self.get_q(next_state, a) for a in self.actions])\n",
        "        old_q = self.get_q(state, action)\n",
        "        new_q = old_q + self.alpha * (reward + self.gamma * max_q_next - old_q)\n",
        "        self.q_table[(state, action)] = new_q\n",
        "\n",
        "def recover_policy(q_table, actions, rows, cols):\n",
        "    \"\"\"\n",
        "    Recover the best policy (mapping of state → optimal action) from a Q-table.\n",
        "\n",
        "    Args:\n",
        "        q_table (dict): A dictionary mapping (state, action) tuples to Q-values.\n",
        "        actions (list): List of all possible actions (strings).\n",
        "        rows (int): Number of rows in the grid environment.\n",
        "        cols (int): Number of columns in the grid environment.\n",
        "\n",
        "    Returns:\n",
        "        dict: A policy mapping each state (tuple) to its best action.\n",
        "    \"\"\"\n",
        "    shape = (rows, cols)\n",
        "    states = []\n",
        "\n",
        "    # === Generate and print all states ===\n",
        "    print(\"### States ###\")\n",
        "    for row in range(rows):\n",
        "        part_states = [(row, col) for col in range(shape[1])]\n",
        "        print(*part_states)\n",
        "        states += part_states\n",
        "\n",
        "    # === Show available actions ===\n",
        "    print(\"\\n### Actions ###\")\n",
        "    print(actions)\n",
        "\n",
        "    policy = {}\n",
        "\n",
        "    # === Determine best action for each state based on Q-values ===\n",
        "    for state in states:\n",
        "        # Get Q-values for each possible action from this state\n",
        "        q_values = [q_table.get((state, a), 0) for a in actions]\n",
        "\n",
        "        # Choose action with highest Q-value\n",
        "        best_action_idx = np.argmax(q_values)\n",
        "        policy[state] = actions[best_action_idx]\n",
        "\n",
        "    return policy\n",
        "\n",
        "\n",
        "def train(agent, env, qlearn, episodes=1000, random_init: bool = False):\n",
        "    \"\"\"\n",
        "    Train an agent in a GridWorld environment using Q-learning.\n",
        "\n",
        "    Args:\n",
        "        agent (Agent): The agent that moves through the environment.\n",
        "        env (GridWorld): The grid-based environment.\n",
        "        qlearn (QLearning): The Q-learning algorithm instance.\n",
        "        episodes (int): Number of episodes to run.\n",
        "        random_init (bool): If True, agent starts at a random position each episode.\n",
        "    \"\"\"\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        # === Initialize Agent ===\n",
        "        if not random_init:\n",
        "            agent.reset((0, 0))  # Default starting point\n",
        "        else:\n",
        "            # Start randomly within inner grid (avoid boundary walls)\n",
        "            agent.reset((random.randint(1, env.rows - 1), random.randint(1, env.cols - 1)))\n",
        "\n",
        "        state = agent.position  # Initial state\n",
        "\n",
        "        for _ in range(env.episode_steps):\n",
        "            # === SAMPLE ACTION ===\n",
        "            action = qlearn.choose_action(state)\n",
        "\n",
        "            # === INTERACT WITH ENVIRONMENT ===\n",
        "            next_state = agent.move(action, env)\n",
        "\n",
        "            # === EVALUATE REWARD ===\n",
        "            # Note: Reward is based on the result of the transition (s, a → s')\n",
        "            # We care about the consequence of the action, not just being in a state\n",
        "            reward = env.get_reward(next_state)\n",
        "\n",
        "            # === LEARNING / Q-VALUE UPDATE ===\n",
        "            qlearn.update(state, action, reward, next_state)\n",
        "\n",
        "            # === PROCEED TO NEXT STATE ===\n",
        "            state = next_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-FDa9CXC5uZ"
      },
      "source": [
        "#Deep Q-Networks (DQN) in Gridworld\n",
        "In this notebook, we extend your understanding of Q-learning by applying it to more complex state representations using neural networks. Tabular Q-learning works well for small, discrete state spaces, but what if your state includes partial observability, local vision, or continuous values?\n",
        "\n",
        "That's where Deep Q-Networks (DQN) come in.\n",
        "\n",
        "🧩 What You'll Build --> A neural network that estimates Q-values directly from features like:\n",
        "\n",
        "- A local grid patch around the agent (to simulate limited perception)\n",
        "\n",
        "- The normalized positions of the agent and the goal\n",
        "\n",
        "- A replay buffer to store past experiences and train the agent more stably\n",
        "\n",
        "- A target network to improve training stability and reduce overestimation\n",
        "\n",
        "- An epsilon-greedy strategy to balance exploration vs. exploitation\n",
        "\n",
        "🔍 Key Concepts Covered\n",
        "\n",
        "- State encoding: Convert your local surroundings and positions into a vector\n",
        "\n",
        "- Q-function approximation: Replace the Q-table with a fully connected neural network\n",
        "\n",
        "- Experience replay: Randomly sample past transitions to break correlations\n",
        "\n",
        "- Target networks: Use a separate network for stable target value estimates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0i-LhkNtXTq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5ee03a6-4335-4d07-c130-46244ec1fed2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.7.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/363.4 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:11\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part 1: Coding env.reset_random()\n",
        "\n",
        "In this task, you’ll implement a method to reset the environment with a new set of randomly placed obstacles and a defined goal position. This is important for training agents that can generalize across different map layouts rather than memorizing a single one.\n",
        "\n",
        "You’ll work with two grid representations:\n",
        "\n",
        "- grid: stores the reward values for each cell.\n",
        "\n",
        "- map_grid: stores the visual/environment features like walls and obstacles (-1), free space (0), or goal (1), which the agent \"sees\"."
      ],
      "metadata": {
        "id": "gO18JFsKsffe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### === Environment: GridWorld === ###\n",
        "class GridWorld:\n",
        "    \"\"\"\n",
        "    A 2D grid environment for reinforcement learning.\n",
        "    Supports goal states, obstacles, boundaries, and rewards.\n",
        "    \"\"\"\n",
        "    def __init__(self, obs: dict, goal: dict, rows: int, cols: int, bound_cost: int = -100, episode_steps: int = 1000):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.bound_cost = bound_cost\n",
        "        self.episode_steps = episode_steps\n",
        "        self.grid = np.full((rows, cols), -1)\n",
        "        self.map_grid = np.full((rows, cols), 0)\n",
        "\n",
        "        # Place obstacles with penalty values (must be > bound_cost)\n",
        "        for (r, c), val in obs.items():\n",
        "            if val <= bound_cost:\n",
        "                raise ValueError(f\"Obstacle value at ({r},{c}) is below or equal to bound_cost ({bound_cost}).\")\n",
        "            self.grid[r, c] = val\n",
        "            self.map_grid[r,c] = -1\n",
        "\n",
        "        # Place goals with positive rewards\n",
        "        for (r, c), val in goal.items():\n",
        "            self.grid[r, c] = val\n",
        "            self.map_grid[r,c] = 1\n",
        "\n",
        "        # Set outer boundaries\n",
        "        self.grid[0, :] = bound_cost\n",
        "        self.grid[-1, :] = bound_cost\n",
        "        self.grid[:, 0] = bound_cost\n",
        "        self.grid[:, -1] = bound_cost\n",
        "\n",
        "        self.map_grid[0, :] = -1\n",
        "        self.map_grid[-1, :] = -1\n",
        "        self.map_grid[:, 0] = -1\n",
        "        self.map_grid[:, -1] = -1\n",
        "\n",
        "        # Cache important locations\n",
        "        self.goal_positions = list(zip(*np.where(self.grid > 0)))\n",
        "        self.obstacle_positions = list(zip(*np.where((self.grid < -1) & (self.grid > bound_cost))))\n",
        "        self.bound_positions = list(zip(*np.where(self.grid == bound_cost)))\n",
        "\n",
        "    def _cache_positions(self):\n",
        "        self.goal_positions = list(zip(*np.where(self.grid == self.goal_reward)))\n",
        "        self.obstacle_positions = list(zip(*np.where(self.grid == self.obstacle_cost)))\n",
        "        self.bound_positions = list(zip(*np.where(self.grid == self.bound_cost)))\n",
        "\n",
        "    def get_reward(self, position):\n",
        "        \"\"\"Return the reward value at a given (row, col) position.\"\"\"\n",
        "        row, col = position\n",
        "        return self.grid[row, col]\n",
        "\n",
        "    #TODO: Reset Random Function!\n",
        "    def reset_random(self, num_obs, goal_pos):\n",
        "      self.grid = np.full((self.rows, self.cols), 0)\n",
        "      self.map_grid = np.full((self.rows, self.cols), 0)\n",
        "\n",
        "      # Set outer boundaries\n",
        "      self.grid[0, :] = self.bound_cost\n",
        "      self.grid[-1, :] = self.bound_cost\n",
        "      self.grid[:, 0] = self.bound_cost\n",
        "      self.grid[:, -1] = self.bound_cost\n",
        "\n",
        "      self.map_grid[0, :] = -1\n",
        "      self.map_grid[-1, :] = -1\n",
        "      self.map_grid[:, 0] = -1\n",
        "      self.map_grid[:, -1] = -1\n",
        "\n",
        "      # TODO: Create a random obstacle list\n",
        "      obs_list = [...]\n",
        "\n",
        "      #TODO: place the obstacles in the grid representations\n",
        "      for i in range(num_obs):\n",
        "        self.grid[...] = -40\n",
        "        self.map_grid[...] = -1\n",
        "\n",
        "      self.grid[goal_pos[0], goal_pos[1]] = 100\n",
        "      self.map_grid[goal_pos[0], goal_pos[1]] = 1\n",
        "\n",
        "      self._cache_positions\n",
        "\n",
        "\n",
        "    def is_terminal(self, position):\n",
        "        \"\"\"Check if a state is a terminal (goal) state.\"\"\"\n",
        "        return position in self.goal_positions\n"
      ],
      "metadata": {
        "id": "Fk0AbK9Dsr_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test your Random Reset!\n",
        "\n",
        "env = GridWorld(obs=obs, goal=goal, rows=9, cols=9)\n",
        "env.reset_random(6, (1,7))\n",
        "print(env.grid)\n",
        "print(env.map_grid)"
      ],
      "metadata": {
        "id": "uMJT1QjAZQ68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 2: Encoding Observations for Deep Q-Network\n",
        "To use a neural network for decision-making, we need to turn the environment state into a numerical vector. In this task, you’ll implement a function that encodes what the agent sees into a fixed-size input for the network.\n",
        "\n",
        "We’ll include:\n",
        "\n",
        "- A local view of the environment around the agent\n",
        "\n",
        "- The agent’s position (normalized)\n",
        "\n",
        "- The goal’s position (normalized)\n",
        "\n",
        "This helps the network focus on relevant context while being flexible to larger or variable-sized grids."
      ],
      "metadata": {
        "id": "aKWYn8Z6tVnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_local_state(agent_pos, goal_pos, grid, range=1):\n",
        "    \"\"\"\n",
        "    Constructs a feature vector:\n",
        "    - Flattened local grid centered on agent, with visual range 'range'\n",
        "    - Agent position\n",
        "    - Goal position\n",
        "\n",
        "    Args:\n",
        "        agent_pos: Tuple (row, col) of agent\n",
        "        goal_pos: Tuple (row, col) of goal\n",
        "        grid: 2D numpy array of the environment, with:\n",
        "          - empty space as 0\n",
        "          - boundaries and obstacles as -1\n",
        "          - goal as positive 1\n",
        "        obs_range: Number of grid cells to look around the agent\n",
        "\n",
        "    Returns:\n",
        "        state_vector: 1D numpy array\n",
        "    \"\"\"\n",
        "    rows, cols = grid.shape\n",
        "    r, c = agent_pos\n",
        "\n",
        "    # Crop local region, handling boundary cases\n",
        "    r_start = max(0, r - range)\n",
        "    r_end = min(rows, r + range + 1)\n",
        "    c_start = max(0, c - range)\n",
        "    c_end = min(cols, c + range + 1)\n",
        "\n",
        "    local = grid[r_start:r_end, c_start:c_end].copy()\n",
        "\n",
        "    # TODO: Pad local region to ensure consistent size\n",
        "    # HINT: How can we make our grid valid at the edges for ANY valid initialization?\n",
        "    # i.e. we want to make a full, blank (-1) grid for our local observation, and then fill it in\n",
        "\n",
        "    padded_local = np.full(..., -1.0) # Pad with boundary value\n",
        "    p_r_start = range - (r - r_start)\n",
        "    p_r_end = p_r_start + (r_end - r_start)\n",
        "    p_c_start = range - (c - c_start)\n",
        "    p_c_end = p_c_start + (c_end - c_start)\n",
        "    padded_local[p_r_start:p_r_end, p_c_start:p_c_end] = local\n",
        "\n",
        "\n",
        "    local_flat = padded_local.flatten()  # shape: (local_size^2,)\n",
        "\n",
        "\n",
        "    # TODO: Normalize positions\n",
        "    # Hint: we normalize by taking our position and dividing it by the width/length of the cage\n",
        "    agent_norm = [...]\n",
        "    goal_r, goal_c = goal_pos\n",
        "    goal_norm = [...]\n",
        "\n",
        "    # TODO: Combine features (hint: use np.concatenate. We want the order to be: our grid, our agent pos, and our goal pos)\n",
        "    # Hint: make sure to return a float arraY (.astype(np.float32))\n",
        "    state_vector = ...\n",
        "    return state_vector\n"
      ],
      "metadata": {
        "id": "1xJ251QftR7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 3: Implementing a Deep Q-Network (DQN)\n",
        "\n",
        "In this task, you'll build the components needed for deep Q-learning:\n",
        "\n",
        "- A neural network (DQN) that approximates the Q-values.\n",
        "\n",
        "- A replay buffer to store past experiences.\n",
        "\n",
        "- A training function to update the policy network using batches from the buffer.\n",
        "\n",
        "These parts form the backbone of the agent’s learning loop."
      ],
      "metadata": {
        "id": "1XUMGXSlwPU2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cEFP9CoEgzG"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "BATCH_SIZE   = 128\n",
        "GAMMA        = 0.99\n",
        "EPS_START    = 0.9\n",
        "EPS_END      = 0.1\n",
        "EPS_DECAY    = 10_000          # step-based exponential decay denominator\n",
        "TAU          = 0.005         # soft target update rate\n",
        "LR           = 3e-4          # AdamW learning rate\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity: int):\n",
        "      #TODO: define the memory buffer -- what data structure works best for this?\n",
        "        self.memory = ...\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size: int):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_observations: int, n_actions: int):\n",
        "        super().__init__()\n",
        "        #TODO: write the architecture of the neural net!\n",
        "        #Hint: our input is n_observations, we want to use 2 hidden layers of size 128, and ReLU activation\n",
        "        #      so: input --> linear --> relu --> linear --> relu --> linear --> output\n",
        "        self.model = nn.Sequential(\n",
        "            ...\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "#TODO: write the explore vs. exploit method which chooses our actions\n",
        "def select_action(state, policy_net, n_actions):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    #Code that calculates how our epsilon should change (scheduling) --> we've done this for you, feel free to change\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1.0 * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "\n",
        "    #write the conditional that allows for epsilon greedy -- note that we exploit if our sample is GREATER than our epsilon threshold\n",
        "    if ...:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state).max(1).indices.view(1, 1)   # greedy\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long) #explore\n",
        "\n",
        "\n",
        "def optimize_model(policy_net, target_net, memory, optimizer):\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return None\n",
        "\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # final states are stored as None\n",
        "    non_final_mask = torch.tensor(tuple(s is not None for s in batch.next_state),\n",
        "                                  device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None], dim=0)\n",
        "\n",
        "    state_batch  = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Q(s_t, a)\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # V(s_{t+1})\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
        "\n",
        "    # expected Q\n",
        "    expected_state_action_values = reward_batch + (GAMMA * next_state_values)\n",
        "    expected_state_action_values = expected_state_action_values.unsqueeze(1)\n",
        "\n",
        "    criterion = nn.SmoothL1Loss()  # Huber\n",
        "    loss = criterion(state_action_values, expected_state_action_values)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 4: Training Loop for DQN\n",
        "In this task, you'll put all the components together to train a Deep Q-Network (DQN) agent to navigate a dynamic grid environment with obstacles.\n",
        "\n",
        "You will:\n",
        "\n",
        "- Sample experiences from the environment.\n",
        "\n",
        "- Encode the local grid state.\n",
        "\n",
        "- Use an epsilon-greedy policy for exploration.\n",
        "\n",
        "- Store transitions in a replay buffer.\n",
        "\n",
        "- Train using mini-batches and periodically update the target network.\n",
        "\n"
      ],
      "metadata": {
        "id": "7PBwICjf9D_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training(env, agent, obs_range=2, num_episodes=600, memory_capacity=1000):\n",
        "    \"\"\"\n",
        "    Training loop for DQN on GridWorld:\n",
        "    - Terminal state or hitting env.episode_steps ends an episode.\n",
        "    - Reward shaping with Manhattan distance bonus.\n",
        "    - Reward clipping to [-1, 1] for stable Q-learning.\n",
        "    \"\"\"\n",
        "    actions = agent.action_space\n",
        "    n_actions = len(actions)\n",
        "\n",
        "    # Observation size\n",
        "    start_pos = (random.randint(1, env.rows - 2), random.randint(1, env.cols - 2))\n",
        "    agent.reset(start_pos)\n",
        "    dummy_state = encode_local_state(agent.position, env.goal_positions[0], env.map_grid, obs_range)\n",
        "    n_observations = dummy_state.shape[0]\n",
        "\n",
        "    #write the code to initialize our policy and target nets -- refer to the class to see what the arguments are. Make sure to put\n",
        "    #   '.to(device) at the end of the method call to make training easier / more computationally efficient\n",
        "    policy_net = ...\n",
        "    target_net = ...\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "    memory = ReplayMemory(memory_capacity)\n",
        "\n",
        "    episode_returns, episode_losses = [], []\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "        # Reset environment\n",
        "        start_pos = (random.randint(1, env.rows - 2), random.randint(1, env.cols - 2))\n",
        "        env.reset_random(num_obs=6, goal_pos=env.goal_positions[0])\n",
        "        agent.reset(start_pos)\n",
        "\n",
        "        # Build initial state\n",
        "        s = encode_local_state(agent.position, env.goal_positions[0], env.map_grid, obs_range)\n",
        "        state = torch.from_numpy(s).float().unsqueeze(0).to(device)\n",
        "\n",
        "        ep_return, losses = 0.0, []\n",
        "        prev_dist = abs(agent.position[0] - env.goal_positions[0][0]) + abs(agent.position[1] - env.goal_positions[0][1])\n",
        "\n",
        "        # Episode loop\n",
        "        for t in range(env.episode_steps):\n",
        "            action_tensor = select_action(state, policy_net, n_actions)\n",
        "            a_idx = action_tensor.item()\n",
        "            a_str = actions[a_idx]\n",
        "\n",
        "            # TODO: Environment step and data collection\n",
        "            next_pos = ...\n",
        "            reward_val = ...\n",
        "            done = ...\n",
        "\n",
        "            # Reward shaping with Manhattan distance bonus -- see if you can figure out why we need this!\n",
        "            new_dist = abs(next_pos[0] - env.goal_positions[0][0]) + abs(next_pos[1] - env.goal_positions[0][1])\n",
        "            shaping_bonus = 0.25 * (prev_dist - new_dist)  # positive if moving closer\n",
        "            reward_val += shaping_bonus\n",
        "            prev_dist = new_dist\n",
        "\n",
        "            # Reward clipping\n",
        "            #reward_val = np.clip(reward_val, -1.0, 1.0) not needed since rewards are manageable\n",
        "            reward = torch.tensor([reward_val], device=device, dtype=torch.float32)\n",
        "\n",
        "            # Next state\n",
        "            if not done:\n",
        "                s_next = encode_local_state(agent.position, env.goal_positions[0], env.map_grid, obs_range)\n",
        "                next_state = torch.from_numpy(s_next).float().unsqueeze(0).to(device)\n",
        "            else:\n",
        "                next_state = None\n",
        "\n",
        "            # Store transition -- WRITE THE CODE to push our transition to the buffer\n",
        "            ...\n",
        "            state = next_state\n",
        "            ep_return += reward_val\n",
        "\n",
        "            # Optimize model\n",
        "            loss_val = optimize_model(policy_net, target_net, memory, optimizer)\n",
        "            if loss_val is not None:\n",
        "                losses.append(loss_val)\n",
        "\n",
        "            # Soft update target network\n",
        "            with torch.no_grad():\n",
        "                for target_param, param in zip(target_net.parameters(), policy_net.parameters()):\n",
        "                    target_param.data.copy_(target_param.data * (1.0 - TAU) + param.data * TAU)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        episode_returns.append(ep_return)\n",
        "        episode_losses.append(np.mean(losses) if losses else 0.0)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {i_episode} | Return: {ep_return:.2f} | \"\n",
        "            f\"Eps(step={steps_done}): {EPS_END + (EPS_START - EPS_END) * math.exp(-1.0 * steps_done / EPS_DECAY):.3f} | \"\n",
        "            f\"AvgLoss: {episode_losses[-1]:.4f}\"\n",
        "        )\n",
        "\n",
        "    return policy_net.state_dict(), episode_returns, episode_losses"
      ],
      "metadata": {
        "id": "a_JiQy5KsXyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biO0ahMZeBKO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training(rewards, losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    color = 'tab:blue'\n",
        "    ax1.set_xlabel('Episode')\n",
        "    ax1.set_ylabel('Total Reward', color=color)\n",
        "    ax1.plot(rewards, color=color, label='Total Reward')\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "    ax1.grid(True)\n",
        "\n",
        "    ax2 = ax1.twinx()  # Create a second y-axis that shares the same x-axis\n",
        "    color = 'tab:red'\n",
        "    ax2.set_ylabel('Avg Loss', color=color)\n",
        "    ax2.plot(losses, color=color, linestyle='--', label='Avg Loss')\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    plt.title(\"DQN Training Progress: Reward vs. Loss per Episode\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "gZ6aSDTFBFZb",
        "outputId": "0ff570a9-4feb-499d-ea9a-91d4a75083bf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'GridWorld' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2330719693.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridWorld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgoal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtrained_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'GridWorld' is not defined"
          ]
        }
      ],
      "source": [
        "#CODE TO RUN THE TRAINING LOOP -- feel free to change parameters\n",
        "\n",
        "goal = {\n",
        "    (1,7): 100\n",
        "}\n",
        "\n",
        "obs = {\n",
        "    (1,1):-50,\n",
        "    (4,3):-50,\n",
        "    (2,4):-50\n",
        "}\n",
        "\n",
        "env = GridWorld(obs=obs, goal=goal, rows=9, cols=9, episode_steps=1000)\n",
        "\n",
        "agent = Agent()\n",
        "trained_dict, rewards, losses = run_training(env, agent, num_episodes=10000)\n",
        "print(rewards)\n",
        "plot_training(rewards, losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "sA31opzJJDko",
        "outputId": "f3ab63b6-052d-4288-f557-7a726a1cd79e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trained_dict' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-4229536108.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0meval_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'trained_dict' is not defined"
          ]
        }
      ],
      "source": [
        "#Evaluate!\n",
        "\n",
        "def eval_dqn(state_dict, env, agent, num_eps, obs_range):\n",
        "    actions = agent.action_space\n",
        "    input_size = (2*obs_range + 1)**2 + 4\n",
        "    num_actions = len(actions)\n",
        "\n",
        "    eval_policy_net = DQN(input_size, num_actions).to(device)\n",
        "    eval_policy_net.load_state_dict(state_dict)\n",
        "    eval_policy_net.eval()\n",
        "    fail_count = 0\n",
        "\n",
        "    for ep in range(num_eps):\n",
        "        print(\"starting episode \", ep)\n",
        "        env.reset_random(6, env.goal_positions[0])\n",
        "        agent.reset((random.randint(1, env.rows-2), random.randint(1, env.cols-2)))\n",
        "        ep_reward = 0.0\n",
        "        fail = False\n",
        "        step = 0\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            while(agent.position not in env.goal_positions):\n",
        "                state = encode_local_state(agent.position, env.goal_positions[0], env.grid, obs_range)\n",
        "                state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "                q_vals = eval_policy_net(state_tensor)\n",
        "                action_idx = q_vals.argmax(1).item()\n",
        "\n",
        "                if(agent.position in env.obstacle_positions or step > 20):\n",
        "                  fail=True\n",
        "                  fail_count = fail_count + 1\n",
        "                  break\n",
        "\n",
        "                action = actions[action_idx]\n",
        "                next_pos = agent.move(action, env)\n",
        "                reward = env.get_reward(next_pos)\n",
        "                ep_reward += reward\n",
        "                step += 1\n",
        "\n",
        "\n",
        "        print(\"episode complete, reward: \", ep_reward, \", failed to solve: \", fail)\n",
        "    print(\"failed to reach goal: \", fail_count)\n",
        "\n",
        "\n",
        "\n",
        "eval_dqn(trained_dict, env, agent, 1000, 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0_tWqF0RxC5"
      },
      "outputs": [],
      "source": [
        "#visualize an episode!\n",
        "from celluloid import Camera\n",
        "from IPython.display import HTML\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def visualize_episode(state_dict, env, agent, obs_range, start_pos=(2, 8), goal_pos=(1,7)):\n",
        "    scale = 1\n",
        "    fig, ax = plt.subplots(figsize=(env.cols * scale, env.rows * scale))\n",
        "    camera = Camera(fig)\n",
        "\n",
        "    ax.set_xlim(0, env.cols)\n",
        "    ax.set_ylim(0, env.rows)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_xticks(np.arange(0, env.cols + 1, 1))\n",
        "    ax.set_yticks(np.arange(0, env.rows + 1, 1))\n",
        "    ax.invert_yaxis()\n",
        "    ax.grid(True)\n",
        "\n",
        "    obs = env.obstacle_positions\n",
        "    bounds = env.bound_positions\n",
        "    goal = env.goal_positions[0]\n",
        "    anim = Move_anim(ax, camera, obs, goal, bounds, invert=True)\n",
        "\n",
        "    actions = agent.action_space\n",
        "    input_size = (2 * obs_range + 1) ** 2 + 4\n",
        "    num_actions = len(actions)\n",
        "\n",
        "    eval_policy_net = DQN(input_size, num_actions).to(device)\n",
        "    eval_policy_net.load_state_dict(state_dict)\n",
        "    eval_policy_net.eval()\n",
        "\n",
        "    env.reset_random(6, goal_pos)\n",
        "    agent.reset(start_pos)\n",
        "    ep_reward = 0\n",
        "    print(\"Goal\", env.goal_positions[0])\n",
        "    print(\"Agent Position\", agent.position)\n",
        "\n",
        "    while agent.position != env.goal_positions[0]:\n",
        "        state = encode_local_state(agent.position, env.goal_positions[0], env.grid, obs_range)\n",
        "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_vals = eval_policy_net(state_tensor)\n",
        "            action_idx = q_vals.argmax(1).item()\n",
        "\n",
        "        action = actions[action_idx]\n",
        "        print(action)\n",
        "\n",
        "        # Animate agent movement\n",
        "        r, c = agent.position\n",
        "        if action == \"right\":\n",
        "            anim.right(c+.5, r+.5)\n",
        "        elif action == \"left\":\n",
        "            anim.left(c+.5, r+.5)\n",
        "        elif action == \"up\":\n",
        "            anim.up(c+.5, r+.5)\n",
        "        elif action == \"down\":\n",
        "            anim.down(c+.5, r+.5)\n",
        "\n",
        "        next_pos = agent.move(action, env)\n",
        "        reward = env.get_reward(next_pos)\n",
        "        ep_reward += reward\n",
        "\n",
        "    print(\"Episode complete, reward:\", ep_reward)\n",
        "    animation = camera.animate()\n",
        "    plt.close(fig)\n",
        "    return HTML(animation.to_html5_video())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqPcsD4AFN5r",
        "outputId": "fb32b344-df2a-48e7-bb73-2b50befaf590"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-100 -100 -100 -100 -100 -100 -100 -100 -100]\n",
            " [-100    0    0    0    0    0    0  100 -100]\n",
            " [-100    0    0  -40    0    0  -40    0 -100]\n",
            " [-100    0    0    0    0    0    0    0 -100]\n",
            " [-100    0  -40    0    0    0    0    0 -100]\n",
            " [-100  -40  -40  -40    0    0    0    0 -100]\n",
            " [-100    0    0    0    0    0    0    0 -100]\n",
            " [-100    0    0    0    0    0    0    0 -100]\n",
            " [-100 -100 -100 -100 -100 -100 -100 -100 -100]]\n",
            "[[-1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
            " [-1  0  0  0  0  0  0  1 -1]\n",
            " [-1  0  0 -1  0  0 -1  0 -1]\n",
            " [-1  0  0  0  0  0  0  0 -1]\n",
            " [-1  0 -1  0  0  0  0  0 -1]\n",
            " [-1 -1 -1 -1  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0 -1]\n",
            " [-1 -1 -1 -1 -1 -1 -1 -1 -1]]\n"
          ]
        }
      ],
      "source": [
        "#Test your Random Reset!\n",
        "\n",
        "env = GridWorld(obs=obs, goal=goal, rows=9, cols=9)\n",
        "env.reset_random(6, (1,7))\n",
        "print(env.grid)\n",
        "print(env.map_grid)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}