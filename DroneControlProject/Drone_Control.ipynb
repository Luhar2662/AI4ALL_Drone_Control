{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8GH6cvSM2UOI"},"outputs":[],"source":["# Required packages and modules for simulating and visualizing Crazyflie drone behavior\n","from quad_utils import plotting, Crazyflie as CF, animate_quad\n","import matplotlib.pyplot as plt  # For plotting drone trajectories and diagnostics\n","import numpy as np               # For numerical operations\n","from scipy.linalg import solve_continuous_are  # For solving the continuous-time algebraic Riccati equation (LQR)\n","from IPython.display import HTML, Image         # For embedding animations and images in the notebook\n"]},{"cell_type":"markdown","metadata":{"id":"RcJtlkei5uzJ"},"source":["###Part 1: Setting up Drone Connection\n","Instructions\n","\n","Before we begin flying or simulating the Crazyflie drone, we need to establish a connection to the correct drone unit. For this project, we will all be using the same drone, which has the group_number 16.\n","\n","\n","Update the group_num variable below with the drone's number,  and run the code block to initiate the connection.\n","\n","If successful, you will start seeing real-time telemetry data printed in the output ‚Äî including roll, pitch, and yaw (orientation angles from the IMU).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PjWvVWwU54Re"},"outputs":[],"source":["group_number ="]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2nF3xPay6YMT","scrolled":true,"outputId":"d2227642-a39c-4f9b-a0c1-66e2b8604038"},"outputs":[{"name":"stdout","output_type":"stream","text":["Scanning interfaces for Crazyflies...\n","Cannot find a Crazyradio Dongle\n","Crazyflies found:\n","No Crazyflies found, cannot run example\n"]}],"source":["# This is an example from the Crazyflie Python API.\n","# See https://github.com/bitcraze/crazyflie-lib-python/blob/master/examples/basiclogSync.py\n","\n","import logging\n","import time\n","\n","import cflib.crtp\n","from cflib.crazyflie import Crazyflie\n","from cflib.crazyflie.log import LogConfig\n","from cflib.crazyflie.syncCrazyflie import SyncCrazyflie\n","from cflib.crazyflie.syncLogger import SyncLogger\n","\n","# Only output errors from the logging framework\n","logging.basicConfig(level=logging.ERROR)\n","\n","\n","# Initialize the low-level drivers (don't list the debug drivers)\n","cflib.crtp.init_drivers(enable_debug_driver=False)\n","# Scan for Crazyflies and use the first one found\n","print('Scanning interfaces for Crazyflies...')\n","available = cflib.crtp.scan_interfaces()\n","print('Crazyflies found:')\n","for i in available:\n","    print(i[0])\n","\n","if len(available) == 0:\n","    print('No Crazyflies found, cannot run example')\n","else:\n","    lg_stab = LogConfig(name='Stabilizer', period_in_ms=10)\n","    lg_stab.add_variable('stabilizer.roll', 'float')\n","    lg_stab.add_variable('stabilizer.pitch', 'float')\n","    lg_stab.add_variable('stabilizer.yaw', 'float')\n","\n","    cf = Crazyflie(rw_cache='./cache')\n","    with SyncCrazyflie(available[0][0], cf=cf) as scf:\n","        with SyncLogger(scf, lg_stab) as logger:\n","            endTime = time.time() + 10\n","\n","            for log_entry in logger:\n","                timestamp = log_entry[0]\n","                data = log_entry[1]\n","                logconf_name = log_entry[2]\n","\n","                print('[%d][%s]: %s' % (timestamp, logconf_name, data))\n","\n","                if time.time() > endTime:\n","                    break"]},{"cell_type":"markdown","source":["#Part 2: Programming Drone Movement\n","\n","###Introduction & Context\n","In this section, you‚Äôll explore how to write Python code to control a Crazyflie drone's physical movement. We‚Äôll start by initializing a connection to the drone, then:\n","\n","1. Wait for accurate position estimates from the onboard Kalman filter.\n","\n","2. Set up the PID controller, which will manage low-level stabilization.\n","\n","3. Command the drone to take off, hover, and follow a custom trajectory made up of waypoints.\n","\n","4. Finally, land and stop the drone safely.\n","\n","This code uses the official Bitcraze Python library for controlling the drone. It's been adapted to let us build high-level movement behaviors by sending position setpoints (i.e. ‚Äúgo to this x, y coordinate‚Äù).\n","\n","You‚Äôll also design a custom helper class, Drone_Controller, which lets you convert high-level movement instructions like \"up\", \"left\", etc., into detailed position commands that the drone can follow.\n","\n","Here are a few helpful functions that are already defined for you:"],"metadata":{"id":"Wbh_8gwV1UvM"}},{"cell_type":"code","source":["# Code adapted from: https://github.com/bitcraze/crazyflie-lib-python/blob/master/examples/autonomousSequence.py\n","\n","import time\n","# CrazyFlie imports:\n","import cflib.crtp\n","from cflib.crazyflie import Crazyflie\n","from cflib.crazyflie.log import LogConfig\n","from cflib.crazyflie.syncCrazyflie import SyncCrazyflie\n","from cflib.crazyflie.syncLogger import SyncLogger\n","\n","## Some helper control functions:\n","## -----------------------------------------------------------------------------------------\n","\n","# Determine initial position:\n","def wait_for_position_estimator(scf):\n","    print('Waiting for estimator to find position...')\n","\n","    log_config = LogConfig(name='Kalman Variance', period_in_ms=500)\n","    log_config.add_variable('kalman.varPX', 'float')\n","    log_config.add_variable('kalman.varPY', 'float')\n","    log_config.add_variable('kalman.varPZ', 'float')\n","\n","    var_y_history = [1000] * 10\n","    var_x_history = [1000] * 10\n","    var_z_history = [1000] * 10\n","\n","    threshold = 0.001\n","    with SyncLogger(scf, log_config) as logger:\n","        for log_entry in logger:\n","            data = log_entry[1]\n","\n","            var_x_history.append(data['kalman.varPX'])\n","            var_x_history.pop(0)\n","            var_y_history.append(data['kalman.varPY'])\n","            var_y_history.pop(0)\n","            var_z_history.append(data['kalman.varPZ'])\n","            var_z_history.pop(0)\n","\n","            min_x = min(var_x_history)\n","            max_x = max(var_x_history)\n","            min_y = min(var_y_history)\n","            max_y = max(var_y_history)\n","            min_z = min(var_z_history)\n","            max_z = max(var_z_history)\n","\n","            print(\"{} {} {}\".\n","                format(max_x - min_x, max_y - min_y, max_z - min_z))\n","\n","            if (max_x - min_x) < threshold and (\n","                    max_y - min_y) < threshold and (\n","                    max_z - min_z) < threshold:\n","                break\n","\n","# Initialize controller:\n","def set_PID_controller(cf):\n","    # Set the PID Controller:\n","    print('Initializing PID Controller')\n","    cf.param.set_value('stabilizer.controller', '1')\n","    cf.param.set_value('kalman.resetEstimation', '1')\n","    time.sleep(0.1)\n","    cf.param.set_value('kalman.resetEstimation', '0')\n","\n","    wait_for_position_estimator(cf)\n","    time.sleep(0.1)\n","    return\n"],"metadata":{"id":"7oKwM-K414TA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Warm-Up: Fill in the Flight Control Functions\n","\n","‚ú® Goal\n","This is a warm-up to help you understand how we use Python to control physical drone behaviors like takeoff, hover, move, and land.\n","\n","You‚Äôll be working with three flight routines:\n","\n","1. ascend_and_hover(cf) ‚Äî makes the drone take off and stay at a fixed height.\n","\n","2. run_sequence(scf, sequence, setpoint_delay) ‚Äî sends the drone through a series of waypoints.\n","\n","3. hover_and_descend(cf) ‚Äî makes the drone hover in place and then land smoothly.\n","\n","Each of these functions is written using the Crazyflie command interface:\n","\n","- cf.commander.send_hover_setpoint(...) controls velocity and height.\n","\n","- cf.commander.send_position_setpoint(...) commands the drone to move to a specific position.\n","\n","```\n","# Hover at 0.5 meters:\n","    for _ in range(30):\n","        cf.commander.send_hover_setpoint(0, 0, 0, 0.5)\n","        time.sleep(0.1)\n","\n","# Go to a position -> (x, y, z, 0.0)\n","cf.commander.send_position_setpoint(position[0],\n","                                    (position[1]),\n","                                      0.5,\n","                                      0.0)\n","    \n","```\n","\n","üß© Your Task\n","You‚Äôll be given incomplete versions of each of these functions. Your job is to:\n","\n","- Use loops to repeat commands over short time intervals.\n","\n","- Adjust the height or position setpoints to achieve smooth motion.\n","\n","- Control time between setpoints using time.sleep().\n","\n","üß† Hint: Drones don‚Äôt respond instantly ‚Äî they need repeated commands at high frequency to maintain stability!\n","\n","\n"],"metadata":{"id":"cGSlKcyT2Q5S"}},{"cell_type":"code","source":["# Ascend and hover:\n","def ascend_and_hover(cf):\n","    # Ascend -- warmup drone:\n","    for y in range(20):\n","        cf.commander.send_hover_setpoint(0, 0, 0, y / 50)\n","        time.sleep(0.1)\n","    # Hover at 0.5 meters:\n","    for _ in range(30):\n","        #TODO: write the command to hover at .5 meters\n","        ...\n","        time.sleep(0.1)\n","    return\n","\n","# Follow the setpoint sequence trajectory:\n","def run_sequence(scf, sequence, setpoint_delay):\n","    cf = scf.cf\n","    # TODO: write the for loop that loops over positions in the sequence\n","    for ... in ...:\n","        print(f'Setting position {(position[0], (position[1]))}')\n","        # \"setpoint delay\" is a parameter to give the drone time to reach the set point\n","        for i in range(setpoint_delay):\n","            ...\n","            time.sleep(0.1)\n","\n","# Hover, descend, and stop all motion:\n","def hover_and_descend(cf):\n","    # Hover at 0.5 meters:\n","    for _ in range(30):\n","        ...\n","        time.sleep(0.1)\n","    # Descend:\n","    for y in range(10):\n","        cf.commander.send_hover_setpoint(0, 0, 0, (10 - y) / 25)\n","        time.sleep(0.1)\n","    # Stop all motion:\n","    for i in range(10):\n","        cf.commander.send_stop_setpoint()\n","        time.sleep(0.1)\n","    return"],"metadata":{"id":"11zaS7KM5lko"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this section, you‚Äôll put together everything you‚Äôve learned so far ‚Äî and take full control of how a Crazyflie drone moves through space by designing and executing your own path.\n","\n","You‚Äôre now working with a flexible and modular framework that allows the drone to:\n","\n","- Interpret a high-level movement plan, written as simple actions like \"up\" or \"left\".\n","\n","- Convert those actions into setpoints that the drone understands ‚Äî real coordinates in meters.\n","\n","- Fly the trajectory autonomously, using the run_setpoint_trajectory() function.\n","\n","\n","üõ† Key Components\n","\n","####üß† Drone_Controller:\n","This class helps bridge the gap between human-readable motion plans and drone-executable coordinates. You now have two ways to create motion:\n","\n","- convert_traj_to_setpoint(...) ‚Äî uses a Trajectory policy to generate a sequence of actions from a start and goal. This will be useful when using our QLearning code later\n","\n","- convert_actions_to_setpoint(...) ‚Äî lets you directly feed in a list of \"up\", \"right\", etc., actions.\n","\n","Both return a list of 2D position setpoints the drone can fly through.\n","\n","\n","üß© Your Task\n","\n","In this warm-up, you‚Äôll be given a partially written version of these functions. You will:\n","\n","- Finish the logic to compute and execute motion sequences.\n","\n","- Choose whether to create your flight plan using a policy (start ‚Üí goal) or a hand-written action list.\n","\n","- Make sure your trajectories are converted into the right format ‚Äî with physical units and smooth transitions."],"metadata":{"id":"1AeU-AIp819c"}},{"cell_type":"code","source":["def run_setpoint_trajectory(group_number, sequence):\n","    # This is the main function to enable the drone to follow the trajectory.\n","\n","    # User inputs:\n","    #\n","    # - group_number: (int) the number corresponding to the drone radio settings.\n","    #\n","    # - sequence: a series of point locations (float) defined as a numpy array, with each row in the following format:\n","    #     [x(meters), y(meters)]\n","    #   Note: the input should be given in drone coordinates (where positive x is forward, and positive y is to the left).\n","    # Example:\n","    # sequence = [\n","    #     [[ 0.          0.        ]\n","    #      [0.18134891  0.08433607]]\n","    #\n","\n","    # Outputs:\n","    # None.\n","\n","    setpoint_delay = 3  # Number of 0.1s steps to spend at each setpoint\n","\n","    # Set the URI the Crazyflie will connect to\n","    uri = f\"radio://0/{group_number}/2M\"\n","\n","    # Initialize Crazyflie radio drivers\n","    cflib.crtp.init_drivers(enable_debug_driver=False)\n","\n","    # Connect to the drone and run the control loop\n","    with SyncCrazyflie(uri, cf=Crazyflie(rw_cache='./cache')) as scf:\n","        cf = scf.cf\n","\n","        # Initialize the PID controller and Kalman estimator\n","        set_PID_controller(cf)\n","\n","        # TODO: Ascend to safe height before moving\n","        ...\n","\n","        # TODO: Run the sequence of setpoints\n","        ...\n","\n","        # TODO: Descend and stop all motion\n","        ...\n","\n","    print(\"Done!\")\n","    return\n","\n","class Drone_Controller:\n","\n","  def __init__(self, grid_width, rows, cols, group_number):\n","      self.grid_width = grid_width\n","      self.rows = rows\n","      self.cols = cols\n","      self.position = np.array([0,0])\n","      self.group_number = group_number\n","\n","  def discretize_move(self, action, start_point, segments):\n","\n","    #TODO: initialize move_seq as a zero array... what shape do we need?\n","    move_seq = ...\n","    cur = start_point\n","    move_seq[0] = cur\n","    increment = ...\n","\n","    #TODO -- implement our discretizing strategy on a move!\n","    # HINT: going \"up\" means positive in the x-value, \"left\" is positive in the y-value\n","    #       Think about the Right hand rule!\n","\n","    if action == \"up\":\n","      for i in range(...):\n","        move_seq[i,0] = ...\n","        move_seq[i,1] = ...\n","\n","    elif action == \"down\":\n","      for i in range(...):\n","        move_seq[i,0] = ...\n","        move_seq[i,1] = ...\n","\n","    elif action == \"left\":\n","      for i in range(...):\n","        move_seq[i,0] = ...\n","        move_seq[i,1] = ...\n","\n","    elif action == \"right\":\n","      for i in range(...):\n","        move_seq[i,0] = ...\n","        move_seq[i,1] = ...\n","    print(\"adding, \", move_seq)\n","    return move_seq\n","\n","\n","  def convert_traj_to_setpoint(self, policy, start, goal, segments, drone_init_pos):\n","      \"\"\"\n","      Converts a list of actions (up/down/left/right) into a sequence of setpoints.\n","\n","      Parameters:\n","      - policy: policy recovered by QLearning\n","      - start: grid position of start in the GridWorld, (x,y)\n","      - goal: grid position of the end in the GridWorld, (x,y)\n","      - segments: the amount by which we discretize each move\n","      - drone_init_pos: the literal start point of the drone, usually (0,0)\n","\n","      \"\"\"\n","      # TODO: make a trajectory and find a list of actions using trajectory.step()\n","      trajectory = Trajectory(policy)\n","      actions = ...\n","\n","      #initialize setpoints\n","      setpoints = np.array(drone_init_pos).reshape((1,2))\n","\n","      # TODO: loop through the actions in our list, and calculate the next set of\n","      #       setpoints. Add them to our setpoints using np.concatenate((array 1, array 2), axis = 0)\n","      # HINT: we need to make sure we start each new setpoint from our last known position.\n","      #       using the index [-1] gets us to the end of a list...\n","      for a in actions:\n","\n","        next_seg = ...\n","        setpoints = ...\n","\n","      return setpoints\n","\n","  def convert_actions_to_setpoint(self, actions, segments, drone_init_pos):\n","      setpoints = np.array(drone_init_pos).reshape((1,2))\n","\n","      # TODO: loop through the actions in our list, and calculate the next set of\n","      #       setpoints. Add them to our setpoints using np.concatenate((array 1, array 2), axis = 0)\n","      # HINT: we need to make sure we start each new setpoint from our last known position.\n","      #       using the index [-1] gets us to the end of a list...\n","\n","      for a in actions:\n","\n","        next_seg = ...\n","        setpoints = ...\n","\n","      return setpoints\n","\n","\n","  def execute_sequence(self, sequence):\n","    run_setpoint_trajectory(self.group_number, sequence)\n","    return\n"],"metadata":{"id":"zTEKUS6q6lIV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's write the code to test our movement protocols with an input list of actions!"],"metadata":{"id":"aEkdCNrp6y15"}},{"cell_type":"code","source":["# Initialize our drone with the correct step size and variables\n","drone = Drone_Controller(...)\n","\n","# Make a sequence using our movement actions:\n","actions = [\"up\", \"left\", \"right\", \"down\"]\n","\n","input = [\"up\", \"right\", \"up\", \"right\"]\n","\n","\n","seq = drone.convert_action_to_setpoint(input, 5, (0,0))\n","\n","drone.execute_sequence(seq)"],"metadata":{"id":"vWqFq_BR60T_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["üß† Reflection Questions\n","\n","1. Why do we break down each action (e.g., \"up\") into multiple small setpoints instead of sending one big jump?\n","\n","2. What‚Äôs the benefit of separating the trajectory generation (convert_actions_to_setpoint) from execution (execute_sequence)?\n","\n","3. What might happen if you forget to ascend the drone before running your waypoint sequence?\n","\n","4. How would you modify discretize_move() to support diagonal or curved motion?\n","\n","5. When might you want to use a Trajectory policy vs. specifying actions directly?"],"metadata":{"id":"26tB55Em9kVm"}},{"cell_type":"markdown","metadata":{"id":"LsMjYtfd-1j1"},"source":["#Part 3: Encoding the Real-World Setup\n","\n","The next part of our project is figuring out how best to use our existing Q-Learning and GridWorld environment to represent the maze our drone must solve in real life. This includes correctly figuring out the scale of the maze, where the obstacles are, and where the drone needs to be. Work in groups in the lab to find these values and model the maze."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"D03_N7jDBEGN","outputId":"37391092-0309-41f4-8c55-e82903e15b26"},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (3970833078.py, line 5)","output_type":"error","traceback":["\u001b[1;36m  Cell \u001b[1;32mIn[4], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    rows, cols =\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"]}],"source":["# Width and height of the rectangular domain:\n","width = 0.0  # total width of the cage\n","height = 0.0 # total height of the cage\n","\n","rows, cols =\n","\n","grid_width =\n","\n","# Obstacles are represented as tuples, where the first element is an np.ndarray containing the center\n","# and the second element is the radius of the obstacle. For example (np.array([3, 4]), 5).\n","# This variable is a list of such tuples.\n","obstacles = []\n","\n","# The goal is represented in the same way as an obstacle.\n","goal = ()\n","\n","# The starting position of the robot.\n","origin = np.array([])"]},{"cell_type":"markdown","metadata":{"id":"iC1VblW9BU3H"},"source":["Below is our implementation of the Q-Learning algorithm from yesterday, with most of the code hidden for space. MAKE SURE TO RUN THIS CODE. A lot of the changes / additions we'll write to the code will be in the form of \"wrapper\" functions, which simply means that we build our new tools to work on top of the existing code that we know works, instead of changing it directly.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x9p9Ae8sBcdX","outputId":"2c70a3fe-d860-4b0d-bed6-72fc7cea7f8d","cellView":"form"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: celluloid in c:\\users\\rahul\\anaconda3\\envs\\mae345\\lib\\site-packages (0.2.0)\n","Requirement already satisfied: matplotlib in c:\\users\\rahul\\anaconda3\\envs\\mae345\\lib\\site-packages (from celluloid) (3.9.2)\n","Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rahul\\anaconda3\\envs\\mae345\\lib\\site-packages (from matplotlib->celluloid) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in c:\\users\\rahul\\anaconda3\\envs\\mae345\\lib\\site-packages (from matplotlib->celluloid) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rahul\\anaconda3\\envs\\mae345\\lib\\site-packages (from matplotlib->celluloid) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\rahul\\anaconda3\\envs\\mae345\\lib\\site-packages (from matplotlib->celluloid) (1.4.4)\n","Requirement already satisfied: numpy>=1.23 in c:\\users\\rahul\\anaconda3\\envs\\mae345\\lib\\site-packages (from matplotlib->celluloid) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\rahul\\anaconda3\\envs\\mae345\\lib\\site-packages (from matplotlib->celluloid) (24.1)\n","Requirement already satisfied: pillow>=8 in c:\\users\\rahul\\anaconda3\\envs\\mae345\\lib\\site-packages (from matplotlib->celluloid) (10.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\rahul\\anaconda3\\envs\\mae345\\lib\\site-packages (from matplotlib->celluloid) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\rahul\\anaconda3\\envs\\mae345\\lib\\site-packages (from matplotlib->celluloid) (2.9.0.post0)\n","Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\rahul\\anaconda3\\envs\\mae345\\lib\\site-packages (from matplotlib->celluloid) (6.4.0)\n","Requirement already satisfied: zipp>=3.1.0 in c:\\users\\rahul\\anaconda3\\envs\\mae345\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->celluloid) (3.17.0)\n","Requirement already satisfied: six>=1.5 in c:\\users\\rahul\\anaconda3\\envs\\mae345\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->celluloid) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# @title\n","from typing import Tuple\n","%pip install celluloid\n","from celluloid import Camera # getting the camera\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from IPython.display import HTML\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from matplotlib.colors import TwoSlopeNorm, Normalize\n","import numpy as np\n","import random\n","\n","\n","### Visualization Tools ###\n","\n","### Visualization Tools ###\n","\n","class Move_anim:\n","    \"\"\"\n","    This class provides functionality for visualizing the motion of a trained\n","    agent in real-time using matplotlib and a camera-like snapshot tool.\n","\n","    Args:\n","        ax_obj (matplotlib axis): The axis to draw the animation on.\n","        cam_obj (camera object): The camera used to capture frames (e.g., celluloid).\n","        obs (list or np.ndarray): List of obstacle coordinates.\n","        goal (list or np.ndarray): Goal coordinate.\n","        bounds (list): Boundary coordinates.\n","        grid_size (float): Size of each grid cell in the animation.\n","        T (float): Time to animate a single move.\n","        invert (bool): Whether to swap row/col in coordinate conversion.\n","    \"\"\"\n","\n","    def __init__(self, ax_obj, cam_obj, obs, goal, bounds, grid_size=1, T=1, invert=False):\n","        self.move_time = T\n","        self.grid_size = grid_size\n","        self.ax = ax_obj\n","        self.camera = cam_obj\n","        self.invert = invert\n","        self._legend_drawn = False\n","\n","        # Ensure obs and goal are numpy arrays\n","        self.obs = np.array(obs) if not isinstance(obs, np.ndarray) else obs\n","        self.goal = np.array(goal) if not isinstance(goal, np.ndarray) else goal\n","        self.bounds = bounds\n","\n","\n","    def right(self, x, y):\n","        \"Animate a rightward move\"\n","        for i in range(int(self.move_time * 10)):\n","            x += self.grid_size / (10 * self.move_time)\n","            self.ax.scatter(x, y, c='black', edgecolors='white', s=120, marker='o')\n","            self.show_obs()\n","            self.camera.snap()\n","        return x, y\n","\n","\n","    def left(self, x, y):\n","        \"Animate a leftward move\"\n","        for i in range(int(self.move_time * 10)):\n","            x -= self.grid_size / (10 * self.move_time)\n","            self.ax.scatter(x, y, c='black', edgecolors='white', s=120, marker='o')\n","            self.show_obs()\n","            self.camera.snap()\n","        return x, y\n","\n","\n","    def up(self, x, y):\n","        \"Animate an upward move\"\n","        for i in range(int(self.move_time * 10)):\n","            y -= self.grid_size / (10 * self.move_time)\n","            self.ax.scatter(x, y, c='black', edgecolors='white', s=120, marker='o')\n","            self.show_obs()\n","            self.camera.snap()\n","        return x, y\n","\n","\n","    def down(self, x, y):\n","        \"Animate a downward move\"\n","        for i in range(int(self.move_time * 10)):\n","            y += self.grid_size / (10 * self.move_time)\n","            self.ax.scatter(x, y, c='black', edgecolors='white', s=120, marker='o')\n","            self.show_obs()\n","            self.camera.snap()\n","        return x, y\n","\n","\n","    def show_obs(self):\n","        \"Show all static objects: obstacles, boundaries, and goal\"\n","        def adapt_coords(pos):\n","            row, col = pos\n","            return (col + 0.5, row + 0.5) if self.invert else (row + 0.5, col + 0.5)\n","\n","        # Draw obstacles (red x)\n","        for pos in self.obs:\n","            x, y = adapt_coords(pos)\n","            self.ax.scatter(x, y, c='red', marker='x', s=100)\n","\n","        # Draw boundaries (black x)\n","        for pos in self.bounds:\n","            x, y = adapt_coords(pos)\n","            self.ax.scatter(x, y, c='black', marker='x', s=100)\n","\n","        # Draw goal (green star)\n","        gx, gy = adapt_coords(self.goal)\n","        self.ax.scatter(gx, gy, c='green', marker='*', s=200)\n","\n","        # Draw legend once\n","        if not self._legend_drawn:\n","            self.ax.scatter([], [], c='red', marker='x', s=100, label='Obstacle')\n","            self.ax.scatter([], [], c='black', marker='x', s=100, label='Boundary')\n","            self.ax.scatter([], [], c='green', marker='*', s=200, label='Goal')\n","            self.ax.scatter([], [], c='black', edgecolors='white', s=120, marker='o', label='Agent')\n","            self._legend_drawn = True\n","\n","    # Execute and animate a full trajectory from (cur_x, cur_y)\n","    def execute_traj(self, traj, cur_x, cur_y):\n","        # Optionally swap row and col based on display preference\n","        if self.invert:\n","            cur_x, cur_y = cur_y, cur_x\n","\n","        # Offset to center agent in the middle of a grid cell\n","        x = cur_x + 0.5\n","        y = cur_y + 0.5\n","\n","        # Move the agent step-by-step based on the trajectory list\n","        for move in traj:\n","            if move == \"right\":\n","                x, y = self.right(x, y)\n","            elif move == \"left\":\n","                x, y = self.left(x, y)\n","            elif move == \"up\":\n","                x, y = self.up(x, y)\n","            elif move == \"down\":\n","                x, y = self.down(x, y)\n","            else:\n","                # No move (could handle invalid move here)\n","                x, y = x, y\n","\n","        # Show legend after the full trajectory is played\n","        self.ax.legend(loc='upper right', fontsize=10)\n","\n","\n","\n","### Method for visualizing q-table with directions corresponding to optimal actions for each state ###\n","\n","def draw_q_grid(q_table, env, scale=1.5, actions=[\"stay\", \"up\", \"down\", \"left\", \"right\"], focus_center=None, focus_size=3):\n","    \"\"\"\n","    Visualizes the Q-values of a grid-based environment using directional arrows and color-coded heatmaps.\n","\n","    Args:\n","        q_table (dict): A dictionary with keys as (state, action) pairs and values as Q-values.\n","        env (GridWorld): The grid environment (used for size and boundary info).\n","        scale (float): Controls the visual scaling of the plot.\n","        actions (list): List of action names corresponding to Q-values.\n","        focus_center (tuple): Optional (x, y) center to zoom into a subsection of the grid.\n","        focus_size (int): Size of the subsection to show if using focus_center.\n","    \"\"\"\n","\n","    # === Determine region to visualize ===\n","    if focus_center:\n","        cx, cy = focus_center\n","        half = focus_size // 2\n","        row_range = range(max(0, cy - half), min(env.rows, cy + half + 1))\n","        col_range = range(max(0, cx - half), min(env.cols, cx + half + 1))\n","    else:\n","        row_range = range(env.rows)\n","        col_range = range(env.cols)\n","\n","    # Arrow offsets for directional visualization\n","    dx, dy = 0.25, 0.25\n","    arrow_dict = {\"U\": (0, -dy), \"D\": (0, dy), \"L\": (-dx, 0), \"R\": (dx, 0)}\n","\n","    # === Initialize plot ===\n","    fig, ax = plt.subplots(figsize=(len(col_range) * scale, len(row_range) * scale))\n","    ax.set_xlim(0, len(col_range))\n","    ax.set_ylim(0, len(row_range))\n","    ax.set_aspect('equal')\n","    ax.set_xticks(np.arange(0, len(col_range)+1, 1))\n","    ax.set_yticks(np.arange(0, len(row_range)+1, 1))\n","    ax.invert_yaxis()\n","    ax.grid(True)\n","\n","    # === Normalize Q-values for color mapping ===\n","    all_q_vals = [v for (_, v) in q_table.items()]\n","    min_q = min(all_q_vals) if all_q_vals else -1\n","    max_q = max(all_q_vals) if all_q_vals else 1\n","    if min_q < 0 and max_q > 0:\n","        norm = TwoSlopeNorm(vmin=min_q, vcenter=0, vmax=max_q)\n","    else:\n","        norm = Normalize(vmin=min_q, vmax=max_q)\n","    cmap = plt.cm.bwr  # Blue-White-Red colormap\n","\n","    # === Iterate over grid cells ===\n","    for i, row in enumerate(row_range):\n","        for j, col in enumerate(col_range):\n","            state = (row, col)\n","\n","            # Get Q-values for each action at this state\n","            q_stay  = q_table.get((state, \"stay\"), 0)\n","            q_up    = q_table.get((state, \"up\"), 0)\n","            q_down  = q_table.get((state, \"down\"), 0)\n","            q_left  = q_table.get((state, \"left\"), 0)\n","            q_right = q_table.get((state, \"right\"), 0)\n","\n","            # Cell center coordinates\n","            x, y = j, i\n","            cx, cy = x + 0.5, y + 0.5\n","\n","            # Identify boundaries\n","            px, py = state\n","            boundary = px == 0 or px == env.rows - 1 or py == 0 or py == env.cols - 1\n","\n","            # === Drawing helper functions ===\n","            def draw_triangle(points, q_val):\n","                color = 'black' if boundary else cmap(norm(q_val))\n","                triangle = patches.Polygon(points, closed=True, facecolor=color, edgecolor='black', alpha=0.85)\n","                ax.add_patch(triangle)\n","\n","            def draw_square(origin):\n","                square = patches.Rectangle(origin, width=1, height=1, facecolor='black', edgecolor='black', alpha=1.0)\n","                ax.add_patch(square)\n","\n","            def draw_stay_circle(center, q_val, radius=0.1):\n","                color = 'black' if (0 in state) or (env.cols-1 in state) else cmap(norm(q_val))\n","                circle = patches.Circle(center, radius, facecolor=color, edgecolor='black', alpha=0.9)\n","                ax.add_patch(circle)\n","\n","            # === Render cell ===\n","            if boundary:\n","                draw_square((x, y))\n","            else:\n","                # Draw directional triangles for each action\n","                draw_triangle([(x, y), (x+1, y), (cx, cy)], q_up)\n","                draw_triangle([(x, y+1), (x+1, y+1), (cx, cy)], q_down)\n","                draw_triangle([(x, y), (x, y+1), (cx, cy)], q_left)\n","                draw_triangle([(x+1, y), (x+1, y+1), (cx, cy)], q_right)\n","\n","                # Draw circle for 'stay' action\n","                draw_stay_circle((cx, cy), q_stay)\n","\n","                # Determine best action and annotate with direction\n","                q_vals = [q_stay, q_up, q_down, q_left, q_right]\n","                best_action = actions[np.argmax(q_vals)]\n","                direction = best_action[0].upper()\n","                ax.text(cx, cy, direction, ha='center', va='center', fontsize=10, color='black')\n","\n","                # Optional: draw directional arrow\n","                if direction in arrow_dict:\n","                    dx, dy = arrow_dict[direction]\n","                    ax.arrow(cx, cy, dx, dy, width=0.006)\n","\n","    # === Add colorbar ===\n","    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n","    sm.set_array([])  # Required for colorbar to work\n","    cbar = plt.colorbar(sm, ax=ax)\n","    cbar.set_label(\"Q-Value\")\n","\n","    # Final formatting\n","    plt.title(\"Q-Value Heatmap\")\n","    plt.tight_layout()\n","    plt.show()\n","\n","    ### Returns optimal action sequence given a policy ###\n","\n","class Trajectory:\n","  \"\"\"\n","  This class provides functionality for extracting or rolling\n","  out optimal trajectory from learned policy.\n","\n","  Args:\n","      policy (dict): A dictionary with keys as state tuples and action strings as values.\n","      action_step (dict): A dictionary with keys as action strings and values as step tuples\n","\n","  \"\"\"\n","\n","  def __init__(self,policy):\n","    self.policy = policy\n","    self.action_step = {\n","        \"up\":(-1,0),\n","        \"down\":(1,0),\n","        \"right\":(0,1),\n","        \"left\":(0,-1),\n","        \"stay\":(0,0)\n","    }\n","\n","\n","\n","  def step(self, start_pos, goal_pos):\n","    \"Recieves starting postion and goal position returns optimal action sequence\"\n","    position = start_pos\n","    actions = []\n","    while not (position == goal):\n","      action = self.policy.get(position)\n","      actions.append(action)\n","      print(action)\n","      step = self.action_step.get(action)\n","      position = tuple(map(sum,zip(position, step)))\n","    return actions\n","\n","#Setup MDP problem\n","\n","\n","\n","### === Environment: GridWorld === ###\n","class GridWorld:\n","    \"\"\"\n","    A 2D grid environment for reinforcement learning.\n","    Supports goal states, obstacles, boundaries, and rewards.\n","    \"\"\"\n","    def __init__(self, obs: dict, goal: dict, rows: int, cols: int, bound_cost: int = -100, episode_steps: int = 1000):\n","        self.rows = rows\n","        self.cols = cols\n","        self.bound_cost = bound_cost\n","        self.episode_steps = episode_steps\n","        self.grid = np.full((rows, cols), 0)\n","\n","        # Place obstacles with penalty values (must be > bound_cost)\n","        for (r, c), val in obs.items():\n","            if val <= bound_cost:\n","                raise ValueError(f\"Obstacle value at ({r},{c}) is below or equal to bound_cost ({bound_cost}).\")\n","            self.grid[r, c] = val\n","\n","        # Place goals with positive rewards\n","        for (r, c), val in goal.items():\n","            self.grid[r, c] = val\n","\n","        # Set outer boundaries\n","        self.grid[0, :] = bound_cost\n","        self.grid[-1, :] = bound_cost\n","        self.grid[:, 0] = bound_cost\n","        self.grid[:, -1] = bound_cost\n","\n","        # Cache important locations\n","        self.goal_positions = list(zip(*np.where(self.grid > 0)))\n","        self.obstacle_positions = list(zip(*np.where((self.grid < -1) & (self.grid > bound_cost))))\n","        self.bound_positions = list(zip(*np.where(self.grid == bound_cost)))\n","\n","    def get_reward(self, position):\n","        \"\"\"Return the reward value at a given (row, col) position.\"\"\"\n","        row, col = position\n","        return self.grid[row, col]\n","\n","    def is_terminal(self, position):\n","        \"\"\"Check if a state is a terminal (goal) state.\"\"\"\n","        return position in self.goal_positions\n","\n","\n","### === Agent === ###\n","class Agent:\n","    \"\"\"\n","    Grid-based agent that can move in 4 directions or stay in place.\n","    \"\"\"\n","    def __init__(self, action_space=[\"stay\", \"up\", \"down\", \"left\", \"right\"]):\n","        self.action_space = action_space\n","\n","    def reset(self, start_pos):\n","        \"\"\"Reset agent to a starting position.\"\"\"\n","        self.position = start_pos\n","\n","    def move(self, action, env: GridWorld):\n","        \"\"\"\n","        Move the agent based on the chosen action, considering environment boundaries.\n","        \"\"\"\n","        row, col = self.position\n","        if action == \"up\" and row > 0:\n","            row -= 1\n","        elif action == \"down\" and row < env.rows - 1:\n","            row += 1\n","        elif action == \"left\" and col > 0:\n","            col -= 1\n","        elif action == \"right\" and col < env.cols - 1:\n","            col += 1\n","        # \"stay\" does nothing\n","        self.position = (row, col)\n","        return self.position\n","\n","### === Q-Learning Algorithm === ###\n","class QLearning:\n","    \"\"\"\n","    Tabular Q-learning agent for discrete state-action environments.\n","    \"\"\"\n","    def __init__(self, env: GridWorld, alpha=0.1, gamma=0.9, epsilon=0.1):\n","        self.alpha = alpha                  # Learning rate\n","        self.gamma = gamma                  # Discount factor\n","        self.epsilon = epsilon              # Exploration rate\n","        self.q_table = {}                   # Stores Q-values: (state, action) -> float\n","        self.actions = [\"stay\", \"up\", \"down\", \"left\", \"right\"]\n","        self.env = env\n","\n","    def get_q(self, state, action):\n","        \"\"\"Return Q-value for a given state-action pair; default to 0.0.\"\"\"\n","        return self.q_table.get((state, action), 0.0)\n","\n","    def choose_action(self, state):\n","        \"\"\"\n","        Choose an action using epsilon-greedy strategy.\n","        With probability epsilon, explore; otherwise exploit.\n","        \"\"\"\n","        if random.random() < self.epsilon:\n","            return random.choice(self.actions)\n","        q_vals = [self.get_q(state, a) for a in self.actions]\n","        return self.actions[np.argmax(q_vals)]\n","\n","    def update(self, state, action, reward, next_state):\n","        \"\"\"\n","        Perform the Q-learning update:\n","        Q(s, a) ‚Üê Q(s, a) + Œ± [r + Œ≥ max_a' Q(s', a') ‚àí Q(s, a)]\n","        \"\"\"\n","\n","        max_q_next = max([self.get_q(next_state, a) for a in self.actions])\n","        old_q = self.get_q(state, action)\n","        new_q = old_q + self.alpha * (reward + self.gamma * max_q_next - old_q)\n","        self.q_table[(state, action)] = new_q\n","\n","def recover_policy(q_table, actions, rows, cols):\n","    \"\"\"\n","    Recover the best policy (mapping of state ‚Üí optimal action) from a Q-table.\n","\n","    Args:\n","        q_table (dict): A dictionary mapping (state, action) tuples to Q-values.\n","        actions (list): List of all possible actions (strings).\n","        rows (int): Number of rows in the grid environment.\n","        cols (int): Number of columns in the grid environment.\n","\n","    Returns:\n","        dict: A policy mapping each state (tuple) to its best action.\n","    \"\"\"\n","    shape = (rows, cols)\n","    states = []\n","\n","    # === Generate and print all states ===\n","    print(\"### States ###\")\n","    for row in range(rows):\n","        part_states = [(row, col) for col in range(shape[1])]\n","        print(*part_states)\n","        states += part_states\n","\n","    # === Show available actions ===\n","    print(\"\\n### Actions ###\")\n","    print(actions)\n","\n","    policy = {}\n","\n","    # === Determine best action for each state based on Q-values ===\n","    for state in states:\n","        # Get Q-values for each possible action from this state\n","        q_values = [q_table.get((state, a), 0) for a in actions]\n","\n","        # Choose action with highest Q-value\n","        best_action_idx = np.argmax(q_values)\n","        policy[state] = actions[best_action_idx]\n","\n","    return policy\n","\n","\n","def train(agent, env, qlearn, episodes=1000, random_init: bool = False):\n","    \"\"\"\n","    Train an agent in a GridWorld environment using Q-learning.\n","\n","    Args:\n","        agent (Agent): The agent that moves through the environment.\n","        env (GridWorld): The grid-based environment.\n","        qlearn (QLearning): The Q-learning algorithm instance.\n","        episodes (int): Number of episodes to run.\n","        random_init (bool): If True, agent starts at a random position each episode.\n","    \"\"\"\n","\n","    for ep in range(episodes):\n","        # === Initialize Agent ===\n","        if not random_init:\n","            agent.reset((0, 0))  # Default starting point\n","        else:\n","            # Start randomly within inner grid (avoid boundary walls)\n","            agent.reset((random.randint(1, env.rows - 1), random.randint(1, env.cols - 1)))\n","\n","        state = agent.position  # Initial state\n","\n","        for _ in range(env.episode_steps):\n","            # === SAMPLE ACTION ===\n","            action = qlearn.choose_action(state)\n","\n","            # === INTERACT WITH ENVIRONMENT ===\n","            next_state = agent.move(action, env)\n","\n","            # === EVALUATE REWARD ===\n","            # Note: Reward is based on the result of the transition (s, a ‚Üí s')\n","            # We care about the consequence of the action, not just being in a state\n","            reward = env.get_reward(next_state)\n","\n","            # === LEARNING / Q-VALUE UPDATE ===\n","            qlearn.update(state, action, reward, next_state)\n","\n","            # === PROCEED TO NEXT STATE ===\n","            state = next_state\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LYgDmGW6ExtU"},"source":["##TODO: Initialize and Train our Policy\n","\n","Use our Q-Learning implementation to solve a representation of the maze we've set up in the lab!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4FnMqoaBwRt"},"outputs":[],"source":["rows, cols = ...\n","\n","obs = ...\n","\n","goal = ...\n","\n","env = ...\n","\n","agent = ...\n","\n","qlearn = ...\n","\n","print(\"Grid Layout\")\n","print(env.grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"srmIglA4FQnD"},"outputs":[],"source":["draw_q_grid(qlearn.q_table, env, scale=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gwg1xBe5FRL8"},"outputs":[],"source":["# TODO: Write the call to train our qlearn algorithm on the environment we've set up\n","\n","# Show the learned value grid\n","draw_q_grid(qlearn.q_table, env, scale=1)\n","\n","# Recover policy\n","policy = recover_policy(qlearn.q_table,qlearn.actions,rows,cols)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YRbuvx3HHrHR"},"outputs":[],"source":["#main - test animation -- all good!\n","scale = 1\n","fig, ax = plt.subplots(figsize=(env.cols * scale, env.rows * scale))\n","\n","camera = Camera(fig)# the camera gets our figure\n","\n","\n","ax.set_xlim(0, env.cols)\n","ax.set_ylim(0, env.rows)\n","ax.set_aspect('equal')\n","ax.set_xticks(np.arange(0, env.cols+1, 1))\n","ax.set_yticks(np.arange(0, env.rows+1, 1))\n","ax.invert_yaxis()\n","ax.grid(True)\n","\n","\n","\n","obs = env.obstacle_positions\n","bounds = env.bound_positions\n","goal = env.goal_positions[0]\n","anim = Move_anim(ax, camera, obs, goal, bounds, invert=True)\n","\n","\n","start = (2,8)\n","trajectories = Trajectory(policy=policy)\n","traj = trajectories.step(start,goal)\n","\n","anim.execute_traj(traj, *start)\n","\n","\n","animation = camera.animate()\n","plt.close()\n","HTML(animation.to_html5_video())"]},{"cell_type":"markdown","metadata":{"id":"Vm0b2vAJFQS5"},"source":["###Controlling the Drone to Solve the Maze:\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YstYCTGJL8I0"},"outputs":[],"source":["#Write the Actual Code to run the drone here\n","# example!\n","\n","rows, cols = 5, 5\n","# Define obstacle and goal positions\n","obs = {\n","    (1, 2): -40,\n","    (3, 3): -20\n","}\n","goal = {\n","    (3, 1): 100\n","}\n","env = GridWorld(obs=obs, goal=goal, rows=rows, cols=cols, episode_steps=100)\n","agent = Agent()\n","qlearn = QLearning(env=env,epsilon=0.8)\n","\n","train(agent,env,qlearn, 100, random_init=True)\n","print(\"train finished\")\n","policy = recover_policy(qlearn.q_table,qlearn.actions,rows,cols)\n","\n","goal = env.goal_positions[0]\n","\n","start = (1,3)\n","trajectories = Trajectory(policy=policy)\n","traj = trajectories.step(start,goal)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cwHnqJ-f0F0k"},"outputs":[],"source":["drone = Drone_Controller(1, 5, 5, group_number)\n","\n","initial = np.array([0,0])\n","\n","seq = drone.convert_traj_to_setpoint(policy, (1,4), (3,1), 5, (0,0))\n","\n","drone.execute_sequence(seq)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1rhCu_vhCVs3SWfNpSrylnmFedpq7JHRa","timestamp":1753202309180}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":0}